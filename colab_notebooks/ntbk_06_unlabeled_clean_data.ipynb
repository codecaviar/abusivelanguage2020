{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ntbk_05_unlabeled_clean_data.ipynb","provenance":[{"file_id":"1CwmCrcjv21d4EfG9rRN8kecdCDcPhJRR","timestamp":1623818503573},{"file_id":"1yC25IXOrqGO0lnPlRLszrNYe2Jqcv65K","timestamp":1623742927300},{"file_id":"1-wIVmkUJdNarVF8fjc0GdfBtx6FgWBUy","timestamp":1623631371169},{"file_id":"1V_EV8ieD4lpbhyX0Ze-MqPKqfbqKoh5Z","timestamp":1623541750199}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qXBhzbcpQaQV"},"source":["# Dive into Abusive Language with Snorkel\n","\n","Author: BingYune Chen \n","<br>\n","Updated: 2021-08-02\n","\n","\n","----------"]},{"cell_type":"markdown","metadata":{"id":"SsoAmwggEYvh"},"source":["### Labeling Functions\n","\n","**Common Types of Labeling Functions:**\n","\n","* Hard-coded heuristics using regular expressions (regexes)\n","* Syntactic analysis using Spacy's dependency trees\n","* Distant supervision based on external knowledge bases (expert labels)\n","* Crowdsourcing noisy manual labels (amateur labels)\n","\n","**We will now apply all of our labeling functions.**"]},{"cell_type":"code","metadata":{"id":"GFPLaKiSoKCH"},"source":["# Imports and setup for Google Colab\n","\n","# Mount Google Drive\n","from google.colab import drive ## module to use Google Drive with Python\n","drive.mount('/content/drive') ## mount to access contents\n","\n","# Install python libraries\n","! pip install --upgrade tensorflow --quiet\n","! pip install snorkel --quiet\n","! pip install tensorboard==1.15.0 --quiet\n","! python -m spacy download en_core_web_sm --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNMOK0VcqCvI"},"source":["# Imports for data and plotting\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","%matplotlib inline \n","import seaborn as sns\n","\n","import pickle\n","import os\n","import re\n","import itertools "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKN7JMcBqPgm"},"source":["# Imports for sentiment analysis\n","# Valence Aware Dictionary and sEntiment Reasoner\n","# VADER was designed with a focus on social media texts\n","import nltk\n","nltk.download('stopwords')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","from nltk.tokenize import regexp_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQl4tnor0g4m"},"source":["# Imports for spaCy preprocessing\n","import spacy\n","spacy.load('en_core_web_sm')\n","spacy.prefer_gpu()\n","from spacy.tokenizer import _get_regex_pattern\n","from spacy.lang.en import English\n","from spacy.matcher import Matcher\n","\n","# Imports for tensorflow/keras preprocessing\n","from keras.models import model_from_json\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oz63_R3qXsQ"},"source":["# Imports for snorkel analysis and multi-task learning\n","from snorkel.preprocess import preprocessor\n","from snorkel.preprocess.nlp import SpacyPreprocessor\n","from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis, model\n","from snorkel.analysis import get_label_buckets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7BbueUV0IMi"},"source":["### Clean Unlabeled Data"]},{"cell_type":"code","metadata":{"id":"aGFET0MPm9j-"},"source":["# Replace contractions \n","# Code adapted from https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597\n","# Contractions source https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n","def load_dict_contractions():\n","    return {\n","        \"ain't\":\"is not\",\n","        \"amn't\":\"am not\",\n","        \"aren't\":\"are not\",\n","        \"can't\":\"cannot\",\n","        \"'cause\":\"because\",\n","        \"couldn't\":\"could not\",\n","        \"couldn't've\":\"could not have\",\n","        \"could've\":\"could have\",\n","        \"daren't\":\"dare not\",\n","        \"daresn't\":\"dare not\",\n","        \"dasn't\":\"dare not\",\n","        \"didn't\":\"did not\",\n","        \"doesn't\":\"does not\",\n","        \"don't\":\"do not\",\n","        \"e'er\":\"ever\",\n","        \"em\":\"them\",\n","        \"everyone's\":\"everyone is\",\n","        \"finna\":\"fixing to\",\n","        \"gimme\":\"give me\",\n","        \"gonna\":\"going to\",\n","        \"gon't\":\"go not\",\n","        \"gotta\":\"got to\",\n","        \"hadn't\":\"had not\",\n","        \"hasn't\":\"has not\",\n","        \"haven't\":\"have not\",\n","        \"he'd\":\"he would\",\n","        \"he'll\":\"he will\",\n","        \"he's\":\"he is\",\n","        \"he've\":\"he have\",\n","        \"how'd\":\"how would\",\n","        \"how'll\":\"how will\",\n","        \"how're\":\"how are\",\n","        \"how's\":\"how is\",\n","        \"I'd\":\"I would\",\n","        \"I'll\":\"I will\",\n","        \"I'm\":\"I am\",\n","        \"I'm'a\":\"I am about to\",\n","        \"I'm'o\":\"I am going to\",\n","        \"isn't\":\"is not\",\n","        \"it'd\":\"it would\",\n","        \"it'll\":\"it will\",\n","        \"it's\":\"it is\",\n","        \"I've\":\"I have\",\n","        \"kinda\":\"kind of\",\n","        \"let's\":\"let us\",\n","        \"mayn't\":\"may not\",\n","        \"may've\":\"may have\",\n","        \"mightn't\":\"might not\",\n","        \"might've\":\"might have\",\n","        \"mustn't\":\"must not\",\n","        \"mustn't've\":\"must not have\",\n","        \"must've\":\"must have\",\n","        \"needn't\":\"need not\",\n","        \"ne'er\":\"never\",\n","        \"o'\":\"of\",\n","        \"o'er\":\"over\",\n","        \"ol'\":\"old\",\n","        \"oughtn't\":\"ought not\",\n","        \"shalln't\":\"shall not\",\n","        \"shan't\":\"shall not\",\n","        \"she'd\":\"she would\",\n","        \"she'll\":\"she will\",\n","        \"she's\":\"she is\",\n","        \"shouldn't\":\"should not\",\n","        \"shouldn't've\":\"should not have\",\n","        \"should've\":\"should have\",\n","        \"somebody's\":\"somebody is\",\n","        \"someone's\":\"someone is\",\n","        \"something's\":\"something is\",\n","        \"that'd\":\"that would\",\n","        \"that'll\":\"that will\",\n","        \"that're\":\"that are\",\n","        \"that's\":\"that is\",\n","        \"there'd\":\"there would\",\n","        \"there'll\":\"there will\",\n","        \"there're\":\"there are\",\n","        \"there's\":\"there is\",\n","        \"these're\":\"these are\",\n","        \"they'd\":\"they would\",\n","        \"they'll\":\"they will\",\n","        \"they're\":\"they are\",\n","        \"they've\":\"they have\",\n","        \"this's\":\"this is\",\n","        \"those're\":\"those are\",\n","        \"'tis\":\"it is\",\n","        \"'twas\":\"it was\",\n","        \"wanna\":\"want to\",\n","        \"wasn't\":\"was not\",\n","        \"we'd\":\"we would\",\n","        \"we'd've\":\"we would have\",\n","        \"we'll\":\"we will\",\n","        \"we're\":\"we are\",\n","        \"weren't\":\"were not\",\n","        \"we've\":\"we have\",\n","        \"what'd\":\"what did\",\n","        \"what'll\":\"what will\",\n","        \"what're\":\"what are\",\n","        \"what's\":\"what is\",\n","        \"what've\":\"what have\",\n","        \"when's\":\"when is\",\n","        \"where'd\":\"where did\",\n","        \"where're\":\"where are\",\n","        \"where's\":\"where is\",\n","        \"where've\":\"where have\",\n","        \"which's\":\"which is\",\n","        \"who'd\":\"who would\",\n","        \"who'd've\":\"who would have\",\n","        \"who'll\":\"who will\",\n","        \"who're\":\"who are\",\n","        \"who's\":\"who is\",\n","        \"who've\":\"who have\",\n","        \"why'd\":\"why did\",\n","        \"why're\":\"why are\",\n","        \"why's\":\"why is\",\n","        \"won't\":\"will not\",\n","        \"wouldn't\":\"would not\",\n","        \"would've\":\"would have\",\n","        \"y'all\":\"you all\",\n","        \"you'd\":\"you would\",\n","        \"you'll\":\"you will\",\n","        \"you're\":\"you are\",\n","        \"you've\":\"you have\",\n","        \"Whatcha\":\"What are you\",\n","        \"luv\":\"love\",\n","        \"sux\":\"sucks\"\n","        }\n","\n","# Clean tweet text to remove mentions, retweets, urls (update to remove \\n text)\n","def clean_tweet_txt(tweet_txt):\n","    \n","    # remove new line tags\n","    tweet_txt = ' '.join(re.sub(\n","        '\\\\n', \n","        ' ', \n","        tweet_txt\n","        ).split()\n","    )\n","\n","    # remove mentions, but keep hashtags\n","    tweet_txt = ' '.join(re.sub(\n","        '(@[A-Za-z0-9_]+\\:)|(@[A-Za-z0-9_\\.]+)', \n","        ' #has_mention ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    # remove retweets\n","    tweet_txt = ' '.join(re.sub(\n","        '(RT\\: )|(RT\\:)|(RT \\: )|(RT )', \n","        ' #has_retweet ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    # remove punctuation not needed for VADER sentiment\n","    tweet_txt = ' '.join(re.sub(\n","        '\\\\.\\\\.\\\\.$|[@â€¦]', \n","        ' #has_truncate ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    # remove urls\n","    tweet_txt = ' '.join(re.sub(\n","        '(\\w+:\\/\\/\\S+)|(\\w+:)', \n","        ' #has_url ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    # expand contractions\n","    CONTRACTIONS = load_dict_contractions()\n","    tweet_txt = tweet_txt.replace(\"â€™\",\"'\")\n","    words = tweet_txt.split()\n","    reformed = [CONTRACTIONS[\n","        word.lower()] if word.lower() in CONTRACTIONS else word \n","        for word in words\n","        ]\n","    tweet_txt = \" \".join(reformed)\n","    \n","    # fix simple misspelled words (character repeats more than 2x)\n","    tweet_txt = ''.join(\n","        ''.join(t)[:2] for _, t in itertools.groupby(tweet_txt)\n","        )\n","    return tweet_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wr-S25IMA-Jl"},"source":["# Apply cleaning function\n","\n","DAY_DIRECTORY = '20201103_english_vf/'\n","OPEN_PATH = (# directory \n","            + DAY_DIRECTORY\n","            )\n","\n","CLEAN_DIRECTORY = '20201103_clean/'\n","SAVE_CLEAN_PATH = (# directory\n","                   + CLEAN_DIRECTORY\n","                   )\n","\n","files = os.listdir(OPEN_PATH)\n","sorted_files = sorted(files)\n","\n","for filename in sorted_files:\n","    if filename.endswith('.txt'):\n","        print('Opening file...{}'.format(filename))\n","        temp_df = pd.read_csv(OPEN_PATH + filename, engine='python')\n","        print('Cleaning file...{}'.format(filename))\n","        temp_df['tweet'] = temp_df['text'].apply(clean_tweet_txt)\n","\n","        os.chdir(SAVE_CLEAN_PATH)\n","        temp_df['tweet'].to_csv('clean_{}'.format(filename), index=False)\n","        print('Saved clean file...{}\\n'.format(filename))\n","\n","    else:\n","        continue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7QQI_DlObvZ"},"source":["# Combine all txt files into a single dataframe\n","\n","CLEAN_DIRECTORY = '20201103_clean/'\n","OPEN_PATH = (# directory\n","             + CLEAN_DIRECTORY\n","            )\n","\n","FULL_FILE_NAME = '20201103'\n","SAVE_CLEAN_PATH = # directory\n","\n","files = os.listdir(OPEN_PATH)\n","sorted_files = sorted(files)\n","\n","main_df = pd.DataFrame(columns=['tweet'])\n","\n","for filename in sorted_files:\n","    if filename.endswith('.txt'):\n","        print('Opening file...{}'.format(filename))\n","        temp_df = pd.read_csv(OPEN_PATH + filename, engine='python')\n","        print('Adding file...{}'.format(filename))\n","        main_df = main_df.append(temp_df, ignore_index=True)\n","    else:\n","        continue\n","\n","os.chdir(SAVE_CLEAN_PATH)\n","main_df.to_csv('clean_{}.txt'.format(FULL_FILE_NAME), index=False)\n","\n","# Number of tweets by day\n","# 11/6 (1254209, 1)\n","# 11/5 (1545942, 1)\n","# 11/4 (1598379, 1) \n","# 11/3 (1558637, 1)\n","# 11/2 \n","# 11/1 (1326398, 1)\n","# 10/31 (1198690, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5zg_D1ZgKxP"},"source":["# Create split for initial data publication\n","FILE_NAME = '20201103_clean_p1.txt'\n","first_split = main_df.sample(n=200000, random_state=42)\n","first_split.to_csv(FILE_NAME)\n","\n","# Create splits for LabelModel processing\n","#chunksize = 25000\n","n = 1\n","for df_chunk in pd.read_csv(FILE_NAME, chunksize=chunksize):\n","    df_chunk.to_csv('{}_{}.txt'.format(FILE_NAME[:-4], n))\n","    n += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MI-fFZFqoDqr"},"source":["### Apply Labeling Functions"]},{"cell_type":"code","metadata":{"id":"ZCOpSvl-KxY2"},"source":["# Set voting values to be used in labeling functions\n","ABSTAIN = -1\n","NO_ABUSE = 0\n","ABUSE = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"000gbs8YvtGu"},"source":["# Make custom preprocessing pipeline for spaCy\n","spacyp = SpacyPreprocessor(\n","    text_field=\"tweet\", \n","    doc_field=\"doc\", \n","    memoize=True,\n","    gpu=True)\n","\n","# Load nltk's English stopwords and add custom tags\n","stops = (nltk.corpus.stopwords.words('english') \n","    + ['#has_mention', '#has_url', '#has_retweet', '#has_truncate']\n","    + ['has_mention', 'has_url', 'has_retweet', 'has_truncate']\n","    )\n","\n","# Additional preprocess step to identify lemmas\n","def spacyp_lemmatize(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stops] \n","    return lemma_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH4cJwQdv-Wj"},"source":["# LFs with Trained Classifiers\n","\n","# Use trained classifier for Tf-Idf bag of words\n","@labeling_function()\n","def lf_clf_tfidf_bow(df_row):\n","    ## load models for prediction\n","    MODEL_PATH = '../models/'\n","    os.chdir(MODEL_PATH)\n","    model_name = 'vectorizer_tfidf.pkl'\n","    with open(model_name, 'rb') as file:\n","        tfidf_vec = pickle.load(file)\n","    with open('model_bow_nb.pkl', 'rb') as file:\n","        model_bow_nb = pickle.load(file)\n","    \n","    ## apply vectorization\n","    X_test_vec = tfidf_vec.transform([df_row.tweet])\n","    ## make prediction\n","    score = model_bow_nb.predict(X_test_vec)\n","\n","    return score[0]\n","\n","# Use trained classifier for word embedding\n","@labeling_function(pre=[spacyp])\n","def lf_clf_wordembed_nlp(df_row):\n","    ## load models for prediction\n","    MODEL_PATH = '../models/'\n","    os.chdir(MODEL_PATH)\n","    with open('detector_bigram.pkl', 'rb') as file:\n","        detector_bigram = pickle.load(file)\n","    with open('detector_trigram.pkl', 'rb') as file:\n","        detector_trigram = pickle.load(file)\n","    with open('token_keras.pkl', 'rb') as file:\n","        k_token = pickle.load(file)\n","\n","    json_file = open('model_wordembed_main.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    loaded_model = model_from_json(loaded_model_json)\n","    loaded_model.load_weights(\"model_wordembed_weights.h5\")\n","\n","    ## detect common bigrams and trigrams\n","    X_test_ug = spacyp_lemmatize(df_row.doc)\n","    X_test_bg = list(detector_bigram[X_test_ug])\n","    X_test_tg = list(detector_trigram[X_test_bg])\n","    ## create sequence\n","    lst_txt2seq = k_token.texts_to_sequences([\" \".join(X_test_tg)])\n","    ## pad sequence\n","    X_test_pad = pad_sequences(\n","        lst_txt2seq,\n","        maxlen=15,\n","        padding=\"post\",\n","        truncating=\"post\"\n","        )\n","    \n","    loaded_model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    optimizer='adam', \n","    metrics=['accuracy']\n","    )\n","\n","    ## make prediction\n","    try: \n","        score = loaded_model(X_test_pad)\n","    except:\n","        score = [0,1] ## default to abuse for oov, more common in dataset\n","    finally:\n","        return np.argmax(score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1eSMe4_z_Lo"},"source":["# LFs with Third-Party Models and Heuristics\n","\n","# Use pre trained model for sentiment analysis\n","# VADER focuses on punctuation, capitalization, degree modifiers, \n","# conjunctions, preceding tri-grams\n","# VADER also accounts for emojis, slang, and emoticons\n","\n","# Positive sentiment, less likely abusive language\n","@labeling_function()\n","def lf_vader_sentiment(df_row):\n","    vader = SentimentIntensityAnalyzer()\n","    vs = vader.polarity_scores(df_row.tweet)['compound']  \n","    if vs >= 0.05: \n","        return NO_ABUSE\n","    elif vs <= -0.05: \n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Load emoji sentiment dictionary\n","emoji_df = pd.read_csv('../data/external/emoji_sentiment_data_v1.csv')\n","emoji_df['sentiment'] = emoji_df[['Positive', 'Neutral', 'Negative']].idxmax(axis=1)\n","pos_emoji = emoji_df.loc[emoji_df['sentiment'] == 'Positive', 'Emoji'] ## positive emoji\n","neg_emoji = emoji_df.loc[emoji_df['sentiment'] == 'Negative', 'Emoji'] ## negative emoji\n","\n","# Positive emoji, less likely abusive language\n","# Apply sentiment ranking based on http://kt.ijs.si/data/Emoji_sentiment_ranking/\n","@labeling_function(pre=[spacyp])\n","def lf_emoji_sentiment_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    \n","    pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n","    neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n","    \n","    matcher.add(\"HAPPY\", pos_patterns)  ## add positive pattern\n","    matcher.add(\"SAD\", neg_patterns)  ## add negative pattern\n","    matches = matcher(df_row.doc)\n","    all_id = ([nlp.vocab.strings[match_id] \n","              for match_id, start, end in matches] \n","              + ['None']\n","              )\n","    result = max(set(all_id), key=all_id.count)\n","    if result == 'HAPPY':\n","        return NO_HATE\n","    elif result == 'SAD':\n","        return HATE\n","    else: \n","        return ABSTAIN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBfUNo7BvzvQ"},"source":["# LFs with Complex Preprocessor spaCy and Pattern Matching\n","\n","# Shorter comment and focus on a person, more likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_person_nlp(df_row):\n","    if len(df_row.doc) < 20 and any(\n","        [ent.label_ == \"PERSON\" for ent in df_row.doc.ents]\n","        ):\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of titles for books, songs, or works of art, more likely abuse \n","@labeling_function(pre=[spacyp])\n","def lf_has_work_art_nlp(df_row):\n","    if any([ent.label_ == \"WORK_OF_ART\" for ent in df_row.doc.ents]):\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of at least 3 named entities, more likely abusive language \n","@labeling_function(pre=[spacyp])\n","def lf_has_3plus_entity_nlp(df_row):\n","    if len([ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\", \"LAW\", \"LANGUAGE\"] \n","            for ent in df_row.doc.ents]\n","           ) > 2:\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of \"please stop\" phrases, sarcastic, more likely abusive language\n","# Usually directed at those making abusive language comments or other issues\n","@labeling_function(pre=[spacyp])\n","def lf_has_please_stop_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"do\"},\n","                {\"LEMMA\": \"not\"}]\n","    pattern2 = [{\"LEMMA\": \"stop\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matcher.add(\"p2\", [pattern2])\n","    matches = matcher(df_row.doc)\n","    return ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Higher stopword ratio, more likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_stopwords_nlp(df_row):\n","    num_stopwords = len(\n","        [True for token in df_row.doc if token.lower_ in stops]\n","        )\n","    ratio  = num_stopwords / len(df_row.doc)\n","    return ABUSE if ratio > 0.5 and len(df_row.doc) > 10 else ABSTAIN\n","\n","# Mentions of \"about harass xyz\" phrases, less likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_harassment_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"harass\"}, {\"LEMMA\": \"me\"}]\n","    pattern2 = [{\"LEMMA\": \"not\"}, {\"LEMMA\": \"harass\"}]\n","    pattern3 = [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"harass\"}]\n","    pattern4 = [{\"LEMMA\": \"about\"}, {\"LEMMA\": \"harass\"}]\n","    pattern5 = [{\"LEMMA\": \"get\"}, {\"LEMMA\": \"harass\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matcher.add(\"p2\", [pattern2])\n","    matcher.add(\"p3\", [pattern3])\n","    matcher.add(\"p4\", [pattern4])\n","    matcher.add(\"p5\", [pattern5])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Mentions of \"report you\" phrases, less likely abusive language \n","@labeling_function(pre=[spacyp])\n","def lf_has_report_you_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"report\"},\n","                {\"LEMMA\": \"you\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Mentions of \"please read\" phrases, less likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_please_read_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern = [{\"LEMMA\": \"please\"},\n","               {\"LEMMA\": \"read\"},\n","               {\"LEMMA\": \"the\", \"OP\": \"?\"},\n","               {\"LEMMA\": \"this\", \"OP\": \"?\"}]\n","    matcher.add(\"p1\", [pattern])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvm2aNerwV9I"},"source":["# Reload unique, offensive word lists from multiple bad word sources\n","\n","bw1 = pd.read_csv('../data/interim/kaggle_hatespeech_detection_badwords2.txt')\n","bw2a = pd.read_csv('../data/interim/kaggle_bad_bad_words2.txt')\n","bw3a = pd.read_csv('../data/interim/badwordslist_badwords2.txt')\n","bw4a = pd.read_csv('../data/interim/profanityfilter_badwords2.txt')\n","bw5a = pd.read_csv('../data/interim/better_profanity_wordlist2.txt')\n","bw6a = pd.read_csv('../data/interim/solo_badwords_map2.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9Cr7BpuxZQk"},"source":["# LFs with Keywords for offsensive words and leetspeak versions\n","\n","# Generalize keyword lookup \n","def keyword_lookup(df_row, keywords, label):\n","    tokens = [token.lower() for token \n","              in regexp_tokenize(df_row.tweet, \"[\\w']+|#[\\w']+\")\n","              ]\n","    if any(word.lower() in tokens for word in keywords):\n","        return label\n","    return ABSTAIN\n","\n","# Use of profanity, racial/ethnic slurs, gender insults, political slurs \n","@labeling_function()\n","def lf_has_bad_words1(df_row):\n","    return keyword_lookup(df_row, \n","                          bw1['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words2(df_row):\n","    return keyword_lookup(df_row, \n","                          bw2a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words3(df_row):\n","    return keyword_lookup(df_row, \n","                          bw3a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words4(df_row):\n","    return keyword_lookup(df_row, \n","                          bw4a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words5(df_row):\n","    return keyword_lookup(df_row, \n","                          bw5a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words6(df_row):\n","    return keyword_lookup(df_row, \n","                          bw6a['word'],\n","                          ABUSE)\n","\n","# Use of please (and variations), sarcastic, more likely abusive language\n","@labeling_function()\n","def lf_has_please(df_row):\n","    return keyword_lookup(df_row, \n","                          ['please', 'plz', 'pls', 'pl'],\n","                          ABUSE)\n","\n","# Use of thank you (and variations), less likely abusive language\n","@labeling_function()\n","def lf_has_thankyou(df_row):\n","    return keyword_lookup(df_row, \n","                          ['thank you', 'thanks', 'thx', 'tx'],\n","                          NO_ABUSE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fovtNj_Rx5oM"},"source":["# LFs with Keywords for specific elements\n","\n","# Use of all CAPS, less likely abusive language\n","@labeling_function()\n","def lf_all_capslock(df_row):\n","    if df_row.tweet == df_row.tweet.upper():\n","        return NO_ABUSE\n","    return ABSTAIN\n","\n","# Has angry punctuations\n","@labeling_function()\n","def lf_has_angry_punctuations(df_row):\n","    return keyword_lookup(df_row, \n","                          ['!!', '??', '**'],\n","                          ABUSE)\n","\n","# Includes url, more likely abusive language\n","@labeling_function()\n","def lf_has_url(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_url'],\n","                          ABUSE)\n","\n","# Includes truncate, longer text, more likely abusive language\n","@labeling_function()\n","def lf_has_truncate(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_truncate'],\n","                          ABUSE)\n","\n","# Includes mention, more likely abusive language\n","@labeling_function()\n","def lf_has_mention(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_mention'],\n","                          ABUSE)\n","\n","# Includes retweet, more likely abusive language\n","@labeling_function()\n","def lf_has_retweet(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_retweet'],\n","                          ABUSE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITTtWM2WRXcF"},"source":["# Define list of labeling functions\n","lfs = [\n","    lf_clf_tfidf_bow,\n","    lf_clf_wordembed_nlp, ## slow word embedding\n","    lf_vader_sentiment,\n","    lf_emoji_sentiment_nlp,\n","    lf_has_person_nlp,\n","    lf_has_work_art_nlp,\n","    lf_has_3plus_entity_nlp,\n","    lf_has_please_stop_nlp,\n","    lf_has_stopwords_nlp,\n","    lf_has_harassment_nlp, ## low coverage and empirical accuracy\n","    lf_has_report_you_nlp, ## low coverage and empirical accuracy of 0%\n","    lf_has_please_read_nlp, ## low coverage and empirical accuracy of 0%\n","    lf_has_bad_words1,\n","    lf_has_bad_words2,\n","    lf_has_bad_words3,\n","    lf_has_bad_words4,\n","    lf_has_bad_words5,\n","    lf_has_bad_words6,\n","    lf_has_please,\n","    lf_has_thankyou,\n","    lf_all_capslock,\n","    lf_has_angry_punctuations, ## low coverage and empirical accuracy of 0%\n","    lf_has_url,\n","    lf_has_truncate,\n","    lf_has_mention,\n","    lf_has_retweet\n","    ]\n","\n","# Setup tooling to analyze labeling functions\n","applier = PandasLFApplier(lfs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":926},"id":"qYW8il_poug2","executionInfo":{"status":"ok","timestamp":1624185486078,"user_tz":420,"elapsed":50929991,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"57e99a17-e2eb-48cf-9820-3bc14a42e11a"},"source":["# Iterate through each chunk of cleaned data\n","OPEN_PATH = ##\n","FILE_NAME = '20201103_clean_p1_1.txt'\n","\n","temp_df = pd.read_csv(OPEN_PATH + FILE_NAME)\n","\n","temp_lfs = applier.apply(temp_df)\n","'''\n","model_name = 'lfs_{}_{}.pkl'.format(FILE_NAME[:8], FILE_NAME[-8:-4])\n","with open(model_name, 'wb') as file:\n","    pickle.dump(temp_lfs, file)\n","'''\n","LFAnalysis(L=temp_lfs, lfs=lfs).lf_summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [14:08:45<00:00,  2.04s/it]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91364</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91364</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.58460</td>\n","      <td>0.58460</td>\n","      <td>0.53992</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.06828</td>\n","      <td>0.06828</td>\n","      <td>0.06528</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.14776</td>\n","      <td>0.14776</td>\n","      <td>0.13744</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.01168</td>\n","      <td>0.01168</td>\n","      <td>0.01048</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.17872</td>\n","      <td>0.17872</td>\n","      <td>0.16436</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.06088</td>\n","      <td>0.06088</td>\n","      <td>0.05664</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.11972</td>\n","      <td>0.11972</td>\n","      <td>0.11096</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[0]</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.06144</td>\n","      <td>0.06144</td>\n","      <td>0.04616</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.09976</td>\n","      <td>0.09976</td>\n","      <td>0.08784</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.00028</td>\n","      <td>0.00028</td>\n","      <td>0.00028</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.00076</td>\n","      <td>0.00076</td>\n","      <td>0.00064</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.00900</td>\n","      <td>0.00900</td>\n","      <td>0.00776</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.00092</td>\n","      <td>0.00092</td>\n","      <td>0.00092</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.02240</td>\n","      <td>0.02240</td>\n","      <td>0.02152</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.00524</td>\n","      <td>0.00524</td>\n","      <td>0.00512</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.00392</td>\n","      <td>0.00392</td>\n","      <td>0.00344</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.36908</td>\n","      <td>0.36908</td>\n","      <td>0.34544</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.31916</td>\n","      <td>0.31916</td>\n","      <td>0.29488</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.80168</td>\n","      <td>0.80168</td>\n","      <td>0.74548</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.53112</td>\n","      <td>0.53112</td>\n","      <td>0.49344</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  Overlaps  Conflicts\n","lf_clf_tfidf_bow            0   [0, 1]   1.00000   1.00000    0.91364\n","lf_clf_wordembed_nlp        1   [0, 1]   1.00000   1.00000    0.91364\n","lf_vader_sentiment          2   [0, 1]   0.58460   0.58460    0.53992\n","lf_emoji_sentiment_nlp      3   [0, 1]   0.06828   0.06828    0.06528\n","lf_has_person_nlp           4      [1]   0.14776   0.14776    0.13744\n","lf_has_work_art_nlp         5      [1]   0.01168   0.01168    0.01048\n","lf_has_3plus_entity_nlp     6      [1]   0.17872   0.17872    0.16436\n","lf_has_please_stop_nlp      7      [1]   0.06088   0.06088    0.05664\n","lf_has_stopwords_nlp        8      [1]   0.11972   0.11972    0.11096\n","lf_has_harassment_nlp       9       []   0.00000   0.00000    0.00000\n","lf_has_report_you_nlp      10       []   0.00000   0.00000    0.00000\n","lf_has_please_read_nlp     11      [0]   0.00004   0.00004    0.00004\n","lf_has_bad_words1          12      [1]   0.06144   0.06144    0.04616\n","lf_has_bad_words2          13      [1]   0.09976   0.09976    0.08784\n","lf_has_bad_words3          14      [1]   0.00028   0.00028    0.00028\n","lf_has_bad_words4          15      [1]   0.00076   0.00076    0.00064\n","lf_has_bad_words5          16      [1]   0.00900   0.00900    0.00776\n","lf_has_bad_words6          17      [1]   0.00092   0.00092    0.00092\n","lf_has_please              18      [1]   0.02240   0.02240    0.02152\n","lf_has_thankyou            19      [0]   0.00524   0.00524    0.00512\n","lf_all_capslock            20      [0]   0.00392   0.00392    0.00344\n","lf_has_angry_punctuations  21       []   0.00000   0.00000    0.00000\n","lf_has_url                 22      [1]   0.36908   0.36908    0.34544\n","lf_has_truncate            23      [1]   0.31916   0.31916    0.29488\n","lf_has_mention             24      [1]   0.80168   0.80168    0.74548\n","lf_has_retweet             25      [1]   0.53112   0.53112    0.49344"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":926},"id":"bENL4Fs2zYvn","executionInfo":{"status":"ok","timestamp":1624250973285,"user_tz":420,"elapsed":50119329,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"a26155e4-3f3e-4aeb-a2db-11c8cb2a6522"},"source":["# Iterate through each chunk of cleaned data\n","OPEN_PATH = ##\n","FILE_NAME = '20201103_clean_p1_4.txt'\n","\n","temp_df = pd.read_csv(OPEN_PATH + FILE_NAME)\n","\n","temp_lfs = applier.apply(temp_df)\n","'''\n","model_name = 'lfs_{}_{}.pkl'.format(FILE_NAME[:8], FILE_NAME[-8:-4])\n","with open(model_name, 'wb') as file:\n","    pickle.dump(temp_lfs, file)\n","'''\n","LFAnalysis(L=temp_lfs, lfs=lfs).lf_summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [13:55:16<00:00,  2.00s/it]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91548</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91548</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.58532</td>\n","      <td>0.58532</td>\n","      <td>0.54136</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.05324</td>\n","      <td>0.05324</td>\n","      <td>0.05240</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.15276</td>\n","      <td>0.15276</td>\n","      <td>0.14224</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.01208</td>\n","      <td>0.01208</td>\n","      <td>0.01096</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.17804</td>\n","      <td>0.17804</td>\n","      <td>0.16496</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.06016</td>\n","      <td>0.06016</td>\n","      <td>0.05648</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.11948</td>\n","      <td>0.11948</td>\n","      <td>0.11020</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[0]</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[0]</td>\n","      <td>0.00028</td>\n","      <td>0.00028</td>\n","      <td>0.00028</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.05912</td>\n","      <td>0.05912</td>\n","      <td>0.04480</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.09776</td>\n","      <td>0.09776</td>\n","      <td>0.08664</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.00032</td>\n","      <td>0.00032</td>\n","      <td>0.00028</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.00896</td>\n","      <td>0.00896</td>\n","      <td>0.00796</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.00088</td>\n","      <td>0.00088</td>\n","      <td>0.00084</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.02276</td>\n","      <td>0.02276</td>\n","      <td>0.02228</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.00556</td>\n","      <td>0.00556</td>\n","      <td>0.00540</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.00476</td>\n","      <td>0.00476</td>\n","      <td>0.00424</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.36788</td>\n","      <td>0.36788</td>\n","      <td>0.34360</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.31620</td>\n","      <td>0.31620</td>\n","      <td>0.29232</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.80496</td>\n","      <td>0.80496</td>\n","      <td>0.74988</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.53008</td>\n","      <td>0.53008</td>\n","      <td>0.49292</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  Overlaps  Conflicts\n","lf_clf_tfidf_bow            0   [0, 1]   1.00000   1.00000    0.91548\n","lf_clf_wordembed_nlp        1   [0, 1]   1.00000   1.00000    0.91548\n","lf_vader_sentiment          2   [0, 1]   0.58532   0.58532    0.54136\n","lf_emoji_sentiment_nlp      3   [0, 1]   0.05324   0.05324    0.05240\n","lf_has_person_nlp           4      [1]   0.15276   0.15276    0.14224\n","lf_has_work_art_nlp         5      [1]   0.01208   0.01208    0.01096\n","lf_has_3plus_entity_nlp     6      [1]   0.17804   0.17804    0.16496\n","lf_has_please_stop_nlp      7      [1]   0.06016   0.06016    0.05648\n","lf_has_stopwords_nlp        8      [1]   0.11948   0.11948    0.11020\n","lf_has_harassment_nlp       9      [0]   0.00004   0.00004    0.00004\n","lf_has_report_you_nlp      10       []   0.00000   0.00000    0.00000\n","lf_has_please_read_nlp     11      [0]   0.00028   0.00028    0.00028\n","lf_has_bad_words1          12      [1]   0.05912   0.05912    0.04480\n","lf_has_bad_words2          13      [1]   0.09776   0.09776    0.08664\n","lf_has_bad_words3          14      [1]   0.00012   0.00012    0.00012\n","lf_has_bad_words4          15      [1]   0.00032   0.00032    0.00028\n","lf_has_bad_words5          16      [1]   0.00896   0.00896    0.00796\n","lf_has_bad_words6          17      [1]   0.00088   0.00088    0.00084\n","lf_has_please              18      [1]   0.02276   0.02276    0.02228\n","lf_has_thankyou            19      [0]   0.00556   0.00556    0.00540\n","lf_all_capslock            20      [0]   0.00476   0.00476    0.00424\n","lf_has_angry_punctuations  21       []   0.00000   0.00000    0.00000\n","lf_has_url                 22      [1]   0.36788   0.36788    0.34360\n","lf_has_truncate            23      [1]   0.31620   0.31620    0.29232\n","lf_has_mention             24      [1]   0.80496   0.80496    0.74988\n","lf_has_retweet             25      [1]   0.53008   0.53008    0.49292"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":926},"id":"P8IUvEuS5-05","executionInfo":{"status":"ok","timestamp":1624314271958,"user_tz":420,"elapsed":14892565,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"7e22b2f4-6e7e-41bc-834f-3b53a62f37ca"},"source":["# Iterate through each chunk of cleaned data\n","OPEN_PATH = ##\n","FILE_NAME = '20201103_clean_p1_6.txt'\n","\n","temp_df = pd.read_csv(OPEN_PATH + FILE_NAME)\n","\n","temp_lfs = applier.apply(temp_df)\n","'''\n","model_name = 'lfs_{}_{}.pkl'.format(FILE_NAME[:8], FILE_NAME[-8:-4])\n","with open(model_name, 'wb') as file:\n","    pickle.dump(temp_lfs, file)\n","'''\n","LFAnalysis(L=temp_lfs, lfs=lfs).lf_summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [17:27:42<00:00,  2.51s/it]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91404</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91404</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.58852</td>\n","      <td>0.58852</td>\n","      <td>0.54160</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.07012</td>\n","      <td>0.07012</td>\n","      <td>0.06684</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.15472</td>\n","      <td>0.15472</td>\n","      <td>0.14460</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.01312</td>\n","      <td>0.01312</td>\n","      <td>0.01192</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.18024</td>\n","      <td>0.18024</td>\n","      <td>0.16620</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.06156</td>\n","      <td>0.06156</td>\n","      <td>0.05724</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.11832</td>\n","      <td>0.11832</td>\n","      <td>0.10780</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[0]</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[0]</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.05960</td>\n","      <td>0.05960</td>\n","      <td>0.04428</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.10016</td>\n","      <td>0.10016</td>\n","      <td>0.08880</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.00032</td>\n","      <td>0.00032</td>\n","      <td>0.00032</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.00052</td>\n","      <td>0.00052</td>\n","      <td>0.00052</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.00920</td>\n","      <td>0.00920</td>\n","      <td>0.00836</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.00136</td>\n","      <td>0.00136</td>\n","      <td>0.00128</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.02344</td>\n","      <td>0.02344</td>\n","      <td>0.02304</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.00508</td>\n","      <td>0.00508</td>\n","      <td>0.00500</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.00432</td>\n","      <td>0.00432</td>\n","      <td>0.00408</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.36508</td>\n","      <td>0.36508</td>\n","      <td>0.34236</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.32076</td>\n","      <td>0.32076</td>\n","      <td>0.29396</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.80376</td>\n","      <td>0.80376</td>\n","      <td>0.74664</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.53288</td>\n","      <td>0.53288</td>\n","      <td>0.49292</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  Overlaps  Conflicts\n","lf_clf_tfidf_bow            0   [0, 1]   1.00000   1.00000    0.91404\n","lf_clf_wordembed_nlp        1   [0, 1]   1.00000   1.00000    0.91404\n","lf_vader_sentiment          2   [0, 1]   0.58852   0.58852    0.54160\n","lf_emoji_sentiment_nlp      3   [0, 1]   0.07012   0.07012    0.06684\n","lf_has_person_nlp           4      [1]   0.15472   0.15472    0.14460\n","lf_has_work_art_nlp         5      [1]   0.01312   0.01312    0.01192\n","lf_has_3plus_entity_nlp     6      [1]   0.18024   0.18024    0.16620\n","lf_has_please_stop_nlp      7      [1]   0.06156   0.06156    0.05724\n","lf_has_stopwords_nlp        8      [1]   0.11832   0.11832    0.10780\n","lf_has_harassment_nlp       9      [0]   0.00012   0.00012    0.00012\n","lf_has_report_you_nlp      10       []   0.00000   0.00000    0.00000\n","lf_has_please_read_nlp     11      [0]   0.00004   0.00004    0.00004\n","lf_has_bad_words1          12      [1]   0.05960   0.05960    0.04428\n","lf_has_bad_words2          13      [1]   0.10016   0.10016    0.08880\n","lf_has_bad_words3          14      [1]   0.00032   0.00032    0.00032\n","lf_has_bad_words4          15      [1]   0.00052   0.00052    0.00052\n","lf_has_bad_words5          16      [1]   0.00920   0.00920    0.00836\n","lf_has_bad_words6          17      [1]   0.00136   0.00136    0.00128\n","lf_has_please              18      [1]   0.02344   0.02344    0.02304\n","lf_has_thankyou            19      [0]   0.00508   0.00508    0.00500\n","lf_all_capslock            20      [0]   0.00432   0.00432    0.00408\n","lf_has_angry_punctuations  21       []   0.00000   0.00000    0.00000\n","lf_has_url                 22      [1]   0.36508   0.36508    0.34236\n","lf_has_truncate            23      [1]   0.32076   0.32076    0.29396\n","lf_has_mention             24      [1]   0.80376   0.80376    0.74664\n","lf_has_retweet             25      [1]   0.53288   0.53288    0.49292"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"JljN2DmukrPv"},"source":["# Iterate through each chunk of cleaned data\n","OPEN_PATH = ##\n","FILE_NAME = '20201103_clean_p1_8.txt'\n","\n","temp_df = pd.read_csv(OPEN_PATH + FILE_NAME)\n","\n","temp_lfs = applier.apply(temp_df)\n","\n","model_name = 'lfs_{}_{}.pkl'.format(FILE_NAME[:8], FILE_NAME[-8:-4])\n","with open(model_name, 'wb') as file:\n","    pickle.dump(temp_lfs, file)\n","\n","LFAnalysis(L=temp_lfs, lfs=lfs).lf_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6gDdQtH0wWX","colab":{"base_uri":"https://localhost:8080/","height":855},"executionInfo":{"status":"ok","timestamp":1624315433029,"user_tz":420,"elapsed":326,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"1db88f69-3060-4108-8104-13c08a2e28b0"},"source":["LFAnalysis(L=tempdf1, lfs=lfs).lf_summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91404</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>0.91404</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.58852</td>\n","      <td>0.58852</td>\n","      <td>0.54160</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.07012</td>\n","      <td>0.07012</td>\n","      <td>0.06684</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.15472</td>\n","      <td>0.15472</td>\n","      <td>0.14460</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.01312</td>\n","      <td>0.01312</td>\n","      <td>0.01192</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.18024</td>\n","      <td>0.18024</td>\n","      <td>0.16620</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.06156</td>\n","      <td>0.06156</td>\n","      <td>0.05724</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.11832</td>\n","      <td>0.11832</td>\n","      <td>0.10780</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[0]</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","      <td>0.00012</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[0]</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.05960</td>\n","      <td>0.05960</td>\n","      <td>0.04428</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.10016</td>\n","      <td>0.10016</td>\n","      <td>0.08880</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.00032</td>\n","      <td>0.00032</td>\n","      <td>0.00032</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.00052</td>\n","      <td>0.00052</td>\n","      <td>0.00052</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.00920</td>\n","      <td>0.00920</td>\n","      <td>0.00836</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.00136</td>\n","      <td>0.00136</td>\n","      <td>0.00128</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.02344</td>\n","      <td>0.02344</td>\n","      <td>0.02304</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.00508</td>\n","      <td>0.00508</td>\n","      <td>0.00500</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.00432</td>\n","      <td>0.00432</td>\n","      <td>0.00408</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.36508</td>\n","      <td>0.36508</td>\n","      <td>0.34236</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.32076</td>\n","      <td>0.32076</td>\n","      <td>0.29396</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.80376</td>\n","      <td>0.80376</td>\n","      <td>0.74664</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.53288</td>\n","      <td>0.53288</td>\n","      <td>0.49292</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  Overlaps  Conflicts\n","lf_clf_tfidf_bow            0   [0, 1]   1.00000   1.00000    0.91404\n","lf_clf_wordembed_nlp        1   [0, 1]   1.00000   1.00000    0.91404\n","lf_vader_sentiment          2   [0, 1]   0.58852   0.58852    0.54160\n","lf_emoji_sentiment_nlp      3   [0, 1]   0.07012   0.07012    0.06684\n","lf_has_person_nlp           4      [1]   0.15472   0.15472    0.14460\n","lf_has_work_art_nlp         5      [1]   0.01312   0.01312    0.01192\n","lf_has_3plus_entity_nlp     6      [1]   0.18024   0.18024    0.16620\n","lf_has_please_stop_nlp      7      [1]   0.06156   0.06156    0.05724\n","lf_has_stopwords_nlp        8      [1]   0.11832   0.11832    0.10780\n","lf_has_harassment_nlp       9      [0]   0.00012   0.00012    0.00012\n","lf_has_report_you_nlp      10       []   0.00000   0.00000    0.00000\n","lf_has_please_read_nlp     11      [0]   0.00004   0.00004    0.00004\n","lf_has_bad_words1          12      [1]   0.05960   0.05960    0.04428\n","lf_has_bad_words2          13      [1]   0.10016   0.10016    0.08880\n","lf_has_bad_words3          14      [1]   0.00032   0.00032    0.00032\n","lf_has_bad_words4          15      [1]   0.00052   0.00052    0.00052\n","lf_has_bad_words5          16      [1]   0.00920   0.00920    0.00836\n","lf_has_bad_words6          17      [1]   0.00136   0.00136    0.00128\n","lf_has_please              18      [1]   0.02344   0.02344    0.02304\n","lf_has_thankyou            19      [0]   0.00508   0.00508    0.00500\n","lf_all_capslock            20      [0]   0.00432   0.00432    0.00408\n","lf_has_angry_punctuations  21       []   0.00000   0.00000    0.00000\n","lf_has_url                 22      [1]   0.36508   0.36508    0.34236\n","lf_has_truncate            23      [1]   0.32076   0.32076    0.29396\n","lf_has_mention             24      [1]   0.80376   0.80376    0.74664\n","lf_has_retweet             25      [1]   0.53288   0.53288    0.49292"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"jFwlmXUkvkid"},"source":[""],"execution_count":null,"outputs":[]}]}