{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ntbk_03b_labeled_build_lf_funcs.ipynb","provenance":[{"file_id":"1CwmCrcjv21d4EfG9rRN8kecdCDcPhJRR","timestamp":1623818503573},{"file_id":"1yC25IXOrqGO0lnPlRLszrNYe2Jqcv65K","timestamp":1623742927300},{"file_id":"1-wIVmkUJdNarVF8fjc0GdfBtx6FgWBUy","timestamp":1623631371169},{"file_id":"1V_EV8ieD4lpbhyX0Ze-MqPKqfbqKoh5Z","timestamp":1623541750199}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qXBhzbcpQaQV"},"source":["# Dive into Abusive Language with Snorkel\n","\n","Author: BingYune Chen \n","<br>\n","Updated: 2021-08-02\n","\n","----------"]},{"cell_type":"markdown","metadata":{"id":"SsoAmwggEYvh"},"source":["### Labeling Functions\n","\n","**Common Types of Labeling Functions:**\n","\n","* Hard-coded heuristics using regular expressions (regexes)\n","* Syntactic analysis using Spacy's dependency trees\n","* Distant supervision based on external knowledge bases (expert labels)\n","* Crowdsourcing noisy manual labels (amateur labels)\n","\n","**We will now build other common types of labeling functions.**"]},{"cell_type":"code","metadata":{"id":"GFPLaKiSoKCH"},"source":["# Imports and setup for Google Colab\n","\n","# Mount Google Drive\n","from google.colab import drive ## module to use Google Drive with Python\n","drive.mount('/content/drive') ## mount to access contents\n","\n","# Install python libraries\n","#! pip install tensorflow-gpu==1.15\n","! pip install --upgrade tensorflow --quiet\n","! pip install snorkel --quiet\n","! pip install tensorboard==1.15.0 --quiet\n","! python -m spacy download en_core_web_sm --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNMOK0VcqCvI"},"source":["# Imports for data and plotting\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","%matplotlib inline \n","import seaborn as sns\n","\n","import pickle\n","import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKN7JMcBqPgm"},"source":["# Imports for sentiment analysis\n","# Valence Aware Dictionary and sEntiment Reasoner\n","# VADER was designed with a focus on social media texts\n","import nltk\n","nltk.download('stopwords')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","from nltk.tokenize import regexp_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQl4tnor0g4m"},"source":["# Imports for spaCy preprocessing\n","import spacy\n","spacy.load('en_core_web_sm')\n","spacy.prefer_gpu()\n","from spacy.tokenizer import _get_regex_pattern\n","from spacy.lang.en import English\n","from spacy.matcher import Matcher\n","\n","# Imports for tensorflow/keras preprocessing\n","from keras.models import model_from_json\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oz63_R3qXsQ"},"source":["# Imports for snorkel analysis and multi-task learning\n","from snorkel.preprocess import preprocessor\n","from snorkel.preprocess.nlp import SpacyPreprocessor\n","from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis, model\n","from snorkel.analysis import get_label_buckets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"EYD4pNZdpH9j","executionInfo":{"status":"ok","timestamp":1624085822691,"user_tz":420,"elapsed":1958,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"bd91e2e1-880c-4ff9-9ee1-01792a89a057"},"source":["# Load labeled dataset for training \n","df_train = pd.read_pickle('../data/processed/df_train.pkl')\n","df_train.reset_index(drop=True, inplace=True)\n","\n","df_dev = pd.read_pickle('../data/processed/df_dev.pkl')\n","df_dev.reset_index(drop=True, inplace=True)\n","\n","df_valid = pd.read_pickle('../data/processed/df_valid.pkl')\n","df_valid.reset_index(drop=True, inplace=True)\n","\n","df_test = pd.read_pickle('../data/processed/df_test.pkl')\n","df_test.reset_index(drop=True, inplace=True)\n","\n","df_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>#has_mention ee you got the hoes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Most of you hoes copy and paste but there has ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>#has_mention only when ur around me 😋 got to k...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>#has_retweet #has_mention *hits blunt* \"bruh i...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>#has_url Alexander Skarsgard, my roommate has ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      1                   #has_mention ee you got the hoes\n","1      1  Most of you hoes copy and paste but there has ...\n","2      1  #has_mention only when ur around me 😋 got to k...\n","3      1  #has_retweet #has_mention *hits blunt* \"bruh i...\n","4      1  #has_url Alexander Skarsgard, my roommate has ..."]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"ZCOpSvl-KxY2"},"source":["# Set voting values to be used in labeling functions\n","ABSTAIN = -1\n","NO_ABUSE = 0\n","ABUSE = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"000gbs8YvtGu"},"source":["# Make custom preprocessing pipeline for spaCy\n","spacyp = SpacyPreprocessor(\n","    text_field=\"tweet\", \n","    doc_field=\"doc\", \n","    memoize=True,\n","    gpu=True)\n","\n","# Load nltk's English stopwords and add custom tags\n","stops = (nltk.corpus.stopwords.words('english') \n","    + ['#has_mention', '#has_url', '#has_retweet', '#has_truncate']\n","    + ['has_mention', 'has_url', 'has_retweet', 'has_truncate']\n","    )\n","\n","# Additional preprocess step to identify lemmas\n","def spacyp_lemmatize(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stops] \n","    return lemma_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH4cJwQdv-Wj"},"source":["# LFs with Trained Classifiers\n","\n","# Use trained classifier for Tf-Idf bag of words\n","@labeling_function()\n","def lf_clf_tfidf_bow(df_row):\n","    ## load models for prediction\n","    MODEL_PATH = '../models/'\n","    os.chdir(MODEL_PATH)\n","    model_name = 'vectorizer_tfidf.pkl'\n","    with open(model_name, 'rb') as file:\n","        tfidf_vec = pickle.load(file)\n","    with open('model_bow_nb.pkl', 'rb') as file:\n","        model_bow_nb = pickle.load(file)\n","    \n","    ## apply vectorization\n","    X_test_vec = tfidf_vec.transform([df_row.tweet])\n","    ## make prediction\n","    score = model_bow_nb.predict(X_test_vec)\n","\n","    return score[0]\n","\n","# Use trained classifier for word embedding\n","@labeling_function(pre=[spacyp])\n","def lf_clf_wordembed_nlp(df_row):\n","    ## load models for prediction\n","    MODEL_PATH = '../models/'\n","    os.chdir(MODEL_PATH)\n","    with open('detector_bigram.pkl', 'rb') as file:\n","        detector_bigram = pickle.load(file)\n","    with open('detector_trigram.pkl', 'rb') as file:\n","        detector_trigram = pickle.load(file)\n","    with open('token_keras.pkl', 'rb') as file:\n","        k_token = pickle.load(file)\n","\n","    json_file = open('model_wordembed_main.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    loaded_model = model_from_json(loaded_model_json)\n","    loaded_model.load_weights(\"model_wordembed_weights.h5\")\n","\n","    ## detect common bigrams and trigrams\n","    X_test_ug = spacyp_lemmatize(df_row.doc)\n","    X_test_bg = list(detector_bigram[X_test_ug])\n","    X_test_tg = list(detector_trigram[X_test_bg])\n","    ## create sequence\n","    lst_txt2seq = k_token.texts_to_sequences([\" \".join(X_test_tg)])\n","    ## pad sequence\n","    X_test_pad = pad_sequences(\n","        lst_txt2seq,\n","        maxlen=15,\n","        padding=\"post\",\n","        truncating=\"post\"\n","        )\n","    \n","    loaded_model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    optimizer='adam', \n","    metrics=['accuracy']\n","    )\n","\n","    ## make prediction\n","    try: \n","        score = loaded_model(X_test_pad)\n","    except:\n","        score = [0,1] ## default to abuse for oov, more common in dataset\n","    finally:\n","        return np.argmax(score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1eSMe4_z_Lo"},"source":["# LFs with Third-Party Models and Heuristics\n","\n","# Use pre trained model for sentiment analysis\n","# VADER focuses on punctuation, capitalization, degree modifiers, \n","# conjunctions, preceding tri-grams\n","# VADER also accounts for emojis, slang, and emoticons\n","\n","# Positive sentiment, less likely abusive language\n","@labeling_function()\n","def lf_vader_sentiment(df_row):\n","    vader = SentimentIntensityAnalyzer()\n","    vs = vader.polarity_scores(df_row.tweet)['compound']  \n","    if vs >= 0.05: \n","        return NO_ABUSE\n","    elif vs <= -0.05: \n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Load emoji sentiment dictionary\n","emoji_df = pd.read_csv('../data/external/emoji_sentiment_data_v1.csv')\n","emoji_df['sentiment'] = emoji_df[['Positive', 'Neutral', 'Negative']].idxmax(axis=1)\n","pos_emoji = emoji_df.loc[emoji_df['sentiment'] == 'Positive', 'Emoji'] ## positive emoji\n","neg_emoji = emoji_df.loc[emoji_df['sentiment'] == 'Negative', 'Emoji'] ## negative emoji\n","\n","# Positive emoji, less likely abusive language\n","# Apply sentiment ranking based on http://kt.ijs.si/data/Emoji_sentiment_ranking/\n","@labeling_function(pre=[spacyp])\n","def lf_emoji_sentiment_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    \n","    pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n","    neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n","    \n","    matcher.add(\"HAPPY\", pos_patterns)  ## add positive pattern\n","    matcher.add(\"SAD\", neg_patterns)  ## add negative pattern\n","    matches = matcher(df_row.doc)\n","    all_id = ([nlp.vocab.strings[match_id] \n","              for match_id, start, end in matches] \n","              + ['None']\n","              )\n","    result = max(set(all_id), key=all_id.count)\n","    if result == 'HAPPY':\n","        return NO_HATE\n","    elif result == 'SAD':\n","        return HATE\n","    else: \n","        return ABSTAIN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBfUNo7BvzvQ"},"source":["# LFs with Complex Preprocessor spaCy and Pattern Matching\n","\n","# Shorter comment and focus on a person, more likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_person_nlp(df_row):\n","    if len(df_row.doc) < 20 and any(\n","        [ent.label_ == \"PERSON\" for ent in df_row.doc.ents]\n","        ):\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of titles for books, songs, or works of art, more likely abuse \n","@labeling_function(pre=[spacyp])\n","def lf_has_work_art_nlp(df_row):\n","    if any([ent.label_ == \"WORK_OF_ART\" for ent in df_row.doc.ents]):\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of at least 3 named entities, more likely abusive language \n","@labeling_function(pre=[spacyp])\n","def lf_has_3plus_entity_nlp(df_row):\n","    if len([ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\", \"LAW\", \"LANGUAGE\"] \n","            for ent in df_row.doc.ents]\n","           ) > 2:\n","        return ABUSE\n","    else:\n","        return ABSTAIN\n","\n","# Mentions of \"please stop\" phrases, sarcastic, more likely abusive language\n","# Usually directed at those making abusive language comments or other issues\n","@labeling_function(pre=[spacyp])\n","def lf_has_please_stop_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"do\"},\n","                {\"LEMMA\": \"not\"}]\n","    pattern2 = [{\"LEMMA\": \"stop\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matcher.add(\"p2\", [pattern2])\n","    matches = matcher(df_row.doc)\n","    return ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Higher stopword ratio, more likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_stopwords_nlp(df_row):\n","    num_stopwords = len(\n","        [True for token in df_row.doc if token.lower_ in stops]\n","        )\n","    ratio  = num_stopwords / len(df_row.doc)\n","    return ABUSE if ratio > 0.5 and len(df_row.doc) > 10 else ABSTAIN\n","\n","# Mentions of \"about harass xyz\" phrases, less likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_harassment_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"harass\"}, {\"LEMMA\": \"me\"}]\n","    pattern2 = [{\"LEMMA\": \"not\"}, {\"LEMMA\": \"harass\"}]\n","    pattern3 = [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"harass\"}]\n","    pattern4 = [{\"LEMMA\": \"about\"}, {\"LEMMA\": \"harass\"}]\n","    pattern5 = [{\"LEMMA\": \"get\"}, {\"LEMMA\": \"harass\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matcher.add(\"p2\", [pattern2])\n","    matcher.add(\"p3\", [pattern3])\n","    matcher.add(\"p4\", [pattern4])\n","    matcher.add(\"p5\", [pattern5])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Mentions of \"report you\" phrases, less likely abusive language \n","@labeling_function(pre=[spacyp])\n","def lf_has_report_you_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern1 = [{\"LEMMA\": \"report\"},\n","                {\"LEMMA\": \"you\"}]\n","    matcher.add(\"p1\", [pattern1])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN\n","\n","# Mentions of \"please read\" phrases, less likely abusive language\n","@labeling_function(pre=[spacyp])\n","def lf_has_please_read_nlp(df_row):\n","    nlp = English()\n","    matcher = Matcher(nlp.vocab)\n","    pattern = [{\"LEMMA\": \"please\"},\n","               {\"LEMMA\": \"read\"},\n","               {\"LEMMA\": \"the\", \"OP\": \"?\"},\n","               {\"LEMMA\": \"this\", \"OP\": \"?\"}]\n","    matcher.add(\"p1\", [pattern])\n","    matches = matcher(df_row.doc)\n","    return NO_ABUSE if len(matches) > 0 else ABSTAIN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvm2aNerwV9I"},"source":["# Make unique, offensive word lists from multiple bad word sources\n","\n","# https://www.kaggle.com/chadapamettapun/hatespeechdetection?select=badWords.csv\n","bw1 = pd.read_csv('../data/external/kaggle_hatespeech_detection_badwords.txt')\n","bw1.drop_duplicates(ignore_index=False, inplace=True)\n","bw1.to_csv('../data/external/kaggle_hatespeech_detection_badwords2.txt', \n","           index=False\n","           )\n","\n","# https://www.kaggle.com/nicapotato/bad-bad-words\n","bw2 = pd.read_csv('../data/external/kaggle_bad_bad_words.txt') ## add word as column name\n","bw2.sort_values(by='word', ignore_index=False, inplace=True)\n","bw2.drop_duplicates(ignore_index=False, inplace=True)\n","bw2['word'] = bw2['word'].str.lower()\n","bw2a = bw1.merge(bw2, how='right', left_on='word', right_on='word')\n","bw2a = bw2a.loc[bw2a['type'].isnull(), 'word'].to_frame()\n","bw2a.to_csv('../data/external/kaggle_bad_bad_words2.txt', index=False)\n","\n","# https://code.google.com/archive/p/badwordslist/downloads\n","bw3 = pd.read_csv('../data/external/badwordslist_badwords.txt') ## add word as column name\n","bw3.sort_values(by='word', ignore_index=False, inplace=True)\n","bw3.drop_duplicates(ignore_index=False, inplace=True)\n","bw3['word'] = bw3['word'].str.lower()\n","bw1bw2a = pd.concat([bw1[['word']], bw2a], ignore_index=False)\n","bw3a = bw1bw2a.merge(bw3, \n","                     how='right', \n","                     left_on='word', \n","                     right_on='word', \n","                     indicator=True)\n","bw3a = bw3a.loc[bw3a['_merge'] == 'right_only', 'word'].to_frame()\n","bw3a.to_csv('../data/external/badwordslist_badwords2.txt', index=False)\n","\n","# https://github.com/areebbeigh/profanityfilter/blob/master/profanityfilter/data/badwords.txt\n","bw4 = pd.read_csv('../data/external/profanityfilter_badwords.txt') ## add word as column name\n","bw4.sort_values(by='word', ignore_index=False, inplace=True)\n","bw4.drop_duplicates(ignore_index=False, inplace=True)\n","bw4['word'] = bw4['word'].str.lower()\n","bw1bw2abw3a = pd.concat([bw1[['word']], bw2a, bw3a], ignore_index=False)\n","bw4a = bw1bw2abw3a.merge(bw4, \n","                         how='right', \n","                         left_on='word', \n","                         right_on='word', \n","                         indicator=True)\n","bw4a = bw4a.loc[bw4a['_merge'] == 'right_only', 'word'].to_frame()\n","bw4a.to_csv('../data/external/profanityfilter_badwords2.txt', index=False)\n","\n","# https://github.com/snguyenthanh/better_profanity/blob/master/better_profanity/profanity_wordlist.txt\n","bw5 = pd.read_csv('../data/external/better_profanity_wordlist.txt') ## add word as column name\n","bw5.sort_values(by='word', ignore_index=False, inplace=True)\n","bw5.drop_duplicates(ignore_index=False, inplace=True)\n","bw5['word'] = bw5['word'].str.lower()\n","bw1bw2abw3abw4a = pd.concat(\n","    [bw1[['word']], bw2a, bw3a, bw4a], \n","    ignore_index=False\n","    )\n","bw5a = bw1bw2abw3abw4a.merge(bw5, \n","                             how='right', \n","                             left_on='word', \n","                             right_on='word', \n","                             indicator=True\n","                             )\n","bw5a = bw5a.loc[bw5a['_merge'] == 'right_only', 'word'].to_frame()\n","bw5a.to_csv('../data/external/better_profanity_wordlist2.txt', index=False)\n","\n","# Additional hashtags and words from various news articles\n","bw6 = pd.read_csv('../data/external/solo_badwords_map.txt') ## add word as column name\n","bw6.sort_values(by='word', ignore_index=False, inplace=True)\n","bw6.drop_duplicates(ignore_index=False, inplace=True)\n","bw6['word'] = bw6['word'].str.lower()\n","bw1bw2abw3abw4abw5a = pd.concat(\n","    [bw1[['word']], bw2a, bw3a, bw4a, bw5a], \n","    ignore_index=False\n","    )\n","bw6a = bw1bw2abw3abw4abw5a.merge(bw6, \n","                                 how='right', \n","                                 left_on='word', \n","                                 right_on='word', \n","                                 indicator=True\n","                                 )\n","bw6a = bw6a.loc[bw6a['_merge'] == 'right_only', 'word'].to_frame() ## no duplicates\n","bw6a.to_csv('../data/external/solo_badwords_map2.txt', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9Cr7BpuxZQk"},"source":["# LFs with Keywords for offsensive words and leetspeak versions\n","\n","# Generalize keyword lookup \n","def keyword_lookup(df_row, keywords, label):\n","    tokens = [token.lower() for token \n","              in regexp_tokenize(df_row.tweet, \"[\\w']+|#[\\w']+\")\n","              ]\n","    if any(word.lower() in tokens for word in keywords):\n","        return label\n","    return ABSTAIN\n","\n","# Use of profanity, racial/ethnic slurs, gender insults, political slurs \n","@labeling_function()\n","def lf_has_bad_words1(df_row):\n","    return keyword_lookup(df_row, \n","                          bw1['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words2(df_row):\n","    return keyword_lookup(df_row, \n","                          bw2a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words3(df_row):\n","    return keyword_lookup(df_row, \n","                          bw3a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words4(df_row):\n","    return keyword_lookup(df_row, \n","                          bw4a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words5(df_row):\n","    return keyword_lookup(df_row, \n","                          bw5a['word'],\n","                          ABUSE)\n","\n","@labeling_function()\n","def lf_has_bad_words6(df_row):\n","    return keyword_lookup(df_row, \n","                          bw6a['word'],\n","                          ABUSE)\n","\n","# Use of please (and variations), sarcastic, more likely abusive language\n","@labeling_function()\n","def lf_has_please(df_row):\n","    return keyword_lookup(df_row, \n","                          ['please', 'plz', 'pls', 'pl'],\n","                          ABUSE)\n","\n","# Use of thank you (and variations), less likely abusive language\n","@labeling_function()\n","def lf_has_thankyou(df_row):\n","    return keyword_lookup(df_row, \n","                          ['thank you', 'thanks', 'thx', 'tx'],\n","                          NO_ABUSE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fovtNj_Rx5oM"},"source":["# LFs with Keywords for specific elements\n","\n","# Use of all CAPS, less likely abusive language\n","@labeling_function()\n","def lf_all_capslock(df_row):\n","    if df_row.tweet == df_row.tweet.upper():\n","        return NO_ABUSE\n","    return ABSTAIN\n","\n","# Has angry punctuations\n","@labeling_function()\n","def lf_has_angry_punctuations(df_row):\n","    return keyword_lookup(df_row, \n","                          ['!!', '??', '**'],\n","                          ABUSE)\n","\n","# Includes url, more likely abusive language\n","@labeling_function()\n","def lf_has_url(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_url'],\n","                          ABUSE)\n","\n","# Includes truncate, longer text, more likely abusive language\n","@labeling_function()\n","def lf_has_truncate(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_truncate'],\n","                          ABUSE)\n","\n","# Includes mention, more likely abusive language\n","@labeling_function()\n","def lf_has_mention(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_mention'],\n","                          ABUSE)\n","\n","# Includes retweet, more likely abusive language\n","@labeling_function()\n","def lf_has_retweet(df_row):\n","    return keyword_lookup(df_row, \n","                          ['#has_retweet'],\n","                          ABUSE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YftKZkPgKJZu"},"source":["### Evaluate LF performance\n","\n","We will calculate the coverage of these labeling functions (LFs). Snorkel provides tooling for common LF analyses using the `LFAnalysis` utility.\n","\n","Table Column Meanings:\n","\n","* **Polarity:** The set of unique labels this labeling function outputs (excluding abstains)\n","* **Coverage:** The fraction of the dataset the label function labels\n","* **Overlaps:** The fraction of the dataset where this labeling function and at least one other labeling function label and agree\n","* **Conflicts:** The fraction of the dataset where this labeling function and at least one other labeling function label and disagree\n","* **Correct:** The number of data points this labeling function labels correctly (does not include abstain)\n","* **Incorrect:** The number of data points this labeling function labels incorrectly (does not include abstain)\n","* **Empirical Accuracy:** The empirical accuracy of this labeling function (does not include abstain)\n","\n","The overall goal is to increase coverage without negatively impacting empiracal accuracy.\n"]},{"cell_type":"code","metadata":{"id":"ITTtWM2WRXcF"},"source":["# Define list of labeling functions\n","lfs = [\n","    lf_clf_tfidf_bow,\n","    lf_clf_wordembed_nlp, ## slower preprocessing time because of tensors\n","    lf_vader_sentiment,\n","    lf_emoji_sentiment_nlp,\n","    lf_has_person_nlp,\n","    lf_has_work_art_nlp,\n","    lf_has_3plus_entity_nlp,\n","    lf_has_please_stop_nlp,\n","    lf_has_stopwords_nlp,\n","    lf_has_harassment_nlp, ## low coverage and empirical accuracy 0%\n","    lf_has_report_you_nlp, ## low coverage and empirical accuracy 0%\n","    lf_has_please_read_nlp, ## low coverage and empirical accuracy 0%\n","    lf_has_bad_words1,\n","    lf_has_bad_words2,\n","    lf_has_bad_words3,\n","    lf_has_bad_words4,\n","    lf_has_bad_words5,\n","    lf_has_bad_words6,\n","    lf_has_please,\n","    lf_has_thankyou,\n","    lf_all_capslock,\n","    lf_has_angry_punctuations, ## low coverage and empirical accuracy 0%\n","    lf_has_url,\n","    lf_has_truncate,\n","    lf_has_mention,\n","    lf_has_retweet\n","    ]\n","\n","# Setup tooling to analyze labeling functions\n","applier = PandasLFApplier(lfs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855},"id":"WB5oTBKE_yxP","executionInfo":{"status":"ok","timestamp":1624085930261,"user_tz":420,"elapsed":190,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"b1c99ccf-ada4-4d51-d7f2-4498c6992700"},"source":["# Create labeling matrix and evaluate results\n","#l_dev = applier.apply(df_dev)\n","\n","# Ignore all future warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","LFAnalysis(L=l_dev, lfs=lfs).lf_summary(Y=df_dev.label.values)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","      <th>Correct</th>\n","      <th>Incorrect</th>\n","      <th>Emp. Acc.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.000</td>\n","      <td>1.000</td>\n","      <td>0.680</td>\n","      <td>154</td>\n","      <td>46</td>\n","      <td>0.770000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.000</td>\n","      <td>1.000</td>\n","      <td>0.680</td>\n","      <td>156</td>\n","      <td>44</td>\n","      <td>0.780000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.735</td>\n","      <td>0.735</td>\n","      <td>0.505</td>\n","      <td>97</td>\n","      <td>50</td>\n","      <td>0.659864</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.070</td>\n","      <td>0.070</td>\n","      <td>0.055</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>0.357143</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.195</td>\n","      <td>0.195</td>\n","      <td>0.135</td>\n","      <td>21</td>\n","      <td>18</td>\n","      <td>0.538462</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.010</td>\n","      <td>0.010</td>\n","      <td>0.005</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.085</td>\n","      <td>0.085</td>\n","      <td>0.075</td>\n","      <td>3</td>\n","      <td>14</td>\n","      <td>0.176471</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.090</td>\n","      <td>0.090</td>\n","      <td>0.075</td>\n","      <td>7</td>\n","      <td>11</td>\n","      <td>0.388889</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.115</td>\n","      <td>0.115</td>\n","      <td>0.090</td>\n","      <td>12</td>\n","      <td>11</td>\n","      <td>0.521739</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.465</td>\n","      <td>0.465</td>\n","      <td>0.210</td>\n","      <td>81</td>\n","      <td>12</td>\n","      <td>0.870968</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.185</td>\n","      <td>0.185</td>\n","      <td>0.125</td>\n","      <td>25</td>\n","      <td>12</td>\n","      <td>0.675676</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.030</td>\n","      <td>0.030</td>\n","      <td>0.010</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.015</td>\n","      <td>0.015</td>\n","      <td>0.015</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>0.005</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.135</td>\n","      <td>0.135</td>\n","      <td>0.125</td>\n","      <td>5</td>\n","      <td>22</td>\n","      <td>0.185185</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.065</td>\n","      <td>0.065</td>\n","      <td>0.050</td>\n","      <td>5</td>\n","      <td>8</td>\n","      <td>0.384615</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.555</td>\n","      <td>0.555</td>\n","      <td>0.405</td>\n","      <td>59</td>\n","      <td>52</td>\n","      <td>0.531532</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.220</td>\n","      <td>0.220</td>\n","      <td>0.165</td>\n","      <td>30</td>\n","      <td>14</td>\n","      <td>0.681818</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  ...  Correct  Incorrect  Emp. Acc.\n","lf_clf_tfidf_bow            0   [0, 1]     1.000  ...      154         46   0.770000\n","lf_clf_wordembed_nlp        1   [0, 1]     1.000  ...      156         44   0.780000\n","lf_vader_sentiment          2   [0, 1]     0.735  ...       97         50   0.659864\n","lf_emoji_sentiment_nlp      3   [0, 1]     0.070  ...        5          9   0.357143\n","lf_has_person_nlp           4      [1]     0.195  ...       21         18   0.538462\n","lf_has_work_art_nlp         5      [1]     0.010  ...        1          1   0.500000\n","lf_has_3plus_entity_nlp     6      [1]     0.085  ...        3         14   0.176471\n","lf_has_please_stop_nlp      7      [1]     0.090  ...        7         11   0.388889\n","lf_has_stopwords_nlp        8      [1]     0.115  ...       12         11   0.521739\n","lf_has_harassment_nlp       9       []     0.000  ...        0          0   0.000000\n","lf_has_report_you_nlp      10       []     0.000  ...        0          0   0.000000\n","lf_has_please_read_nlp     11       []     0.000  ...        0          0   0.000000\n","lf_has_bad_words1          12      [1]     0.465  ...       81         12   0.870968\n","lf_has_bad_words2          13      [1]     0.185  ...       25         12   0.675676\n","lf_has_bad_words3          14       []     0.000  ...        0          0   0.000000\n","lf_has_bad_words4          15       []     0.000  ...        0          0   0.000000\n","lf_has_bad_words5          16      [1]     0.030  ...        5          1   0.833333\n","lf_has_bad_words6          17      [1]     0.015  ...        0          3   0.000000\n","lf_has_please              18      [1]     0.005  ...        0          1   0.000000\n","lf_has_thankyou            19      [0]     0.005  ...        0          1   0.000000\n","lf_all_capslock            20      [0]     0.005  ...        1          0   1.000000\n","lf_has_angry_punctuations  21       []     0.000  ...        0          0   0.000000\n","lf_has_url                 22      [1]     0.135  ...        5         22   0.185185\n","lf_has_truncate            23      [1]     0.065  ...        5          8   0.384615\n","lf_has_mention             24      [1]     0.555  ...       59         52   0.531532\n","lf_has_retweet             25      [1]     0.220  ...       30         14   0.681818\n","\n","[26 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Vdef_Cc7ALrn"},"source":["# Save the unigram transformer for prediction\n","SAVE_PATH = '../models/'\n","\n","# Set save directory for transformer unigram\n","os.chdir(SAVE_PATH)\n","#model_name = 'lf_dev_final.pkl'\n","with open(model_name, 'wb') as file:\n","    pickle.dump(l_dev, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855},"id":"iLIU9W_0EsTW","executionInfo":{"status":"ok","timestamp":1624085962364,"user_tz":420,"elapsed":158,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"b9cbf0a8-b2d8-41c0-db35-ab42bf0290ac"},"source":["# Create labeling matrix and evaluate results\n","#l_valid = applier.apply(df_valid)\n","\n","# Ignore all future warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","LFAnalysis(L=l_valid, lfs=lfs).lf_summary(Y=df_valid.label.values)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","      <th>Correct</th>\n","      <th>Incorrect</th>\n","      <th>Emp. Acc.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.599432</td>\n","      <td>585</td>\n","      <td>119</td>\n","      <td>0.830966</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.599432</td>\n","      <td>533</td>\n","      <td>171</td>\n","      <td>0.757102</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.794034</td>\n","      <td>0.794034</td>\n","      <td>0.461648</td>\n","      <td>404</td>\n","      <td>155</td>\n","      <td>0.722719</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.095170</td>\n","      <td>0.095170</td>\n","      <td>0.080966</td>\n","      <td>17</td>\n","      <td>50</td>\n","      <td>0.253731</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.213068</td>\n","      <td>0.213068</td>\n","      <td>0.139205</td>\n","      <td>89</td>\n","      <td>61</td>\n","      <td>0.593333</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.008523</td>\n","      <td>0.008523</td>\n","      <td>0.004261</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.051136</td>\n","      <td>0.051136</td>\n","      <td>0.041193</td>\n","      <td>15</td>\n","      <td>21</td>\n","      <td>0.416667</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.105114</td>\n","      <td>0.105114</td>\n","      <td>0.062500</td>\n","      <td>51</td>\n","      <td>23</td>\n","      <td>0.689189</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.115057</td>\n","      <td>0.115057</td>\n","      <td>0.068182</td>\n","      <td>58</td>\n","      <td>23</td>\n","      <td>0.716049</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.592330</td>\n","      <td>0.592330</td>\n","      <td>0.242898</td>\n","      <td>400</td>\n","      <td>17</td>\n","      <td>0.959233</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.159091</td>\n","      <td>0.159091</td>\n","      <td>0.098011</td>\n","      <td>84</td>\n","      <td>28</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.048295</td>\n","      <td>0.048295</td>\n","      <td>0.021307</td>\n","      <td>29</td>\n","      <td>5</td>\n","      <td>0.852941</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.002841</td>\n","      <td>0.002841</td>\n","      <td>0.002841</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.011364</td>\n","      <td>0.011364</td>\n","      <td>0.005682</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.012784</td>\n","      <td>0.012784</td>\n","      <td>0.012784</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>0.555556</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.002841</td>\n","      <td>0.002841</td>\n","      <td>0.001420</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.130682</td>\n","      <td>0.130682</td>\n","      <td>0.105114</td>\n","      <td>44</td>\n","      <td>48</td>\n","      <td>0.478261</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.075284</td>\n","      <td>0.075284</td>\n","      <td>0.052557</td>\n","      <td>39</td>\n","      <td>14</td>\n","      <td>0.735849</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.593750</td>\n","      <td>0.593750</td>\n","      <td>0.396307</td>\n","      <td>298</td>\n","      <td>120</td>\n","      <td>0.712919</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.241477</td>\n","      <td>0.241477</td>\n","      <td>0.144886</td>\n","      <td>140</td>\n","      <td>30</td>\n","      <td>0.823529</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  ...  Correct  Incorrect  Emp. Acc.\n","lf_clf_tfidf_bow            0   [0, 1]  1.000000  ...      585        119   0.830966\n","lf_clf_wordembed_nlp        1   [0, 1]  1.000000  ...      533        171   0.757102\n","lf_vader_sentiment          2   [0, 1]  0.794034  ...      404        155   0.722719\n","lf_emoji_sentiment_nlp      3   [0, 1]  0.095170  ...       17         50   0.253731\n","lf_has_person_nlp           4      [1]  0.213068  ...       89         61   0.593333\n","lf_has_work_art_nlp         5      [1]  0.008523  ...        4          2   0.666667\n","lf_has_3plus_entity_nlp     6      [1]  0.051136  ...       15         21   0.416667\n","lf_has_please_stop_nlp      7      [1]  0.105114  ...       51         23   0.689189\n","lf_has_stopwords_nlp        8      [1]  0.115057  ...       58         23   0.716049\n","lf_has_harassment_nlp       9       []  0.000000  ...        0          0   0.000000\n","lf_has_report_you_nlp      10       []  0.000000  ...        0          0   0.000000\n","lf_has_please_read_nlp     11       []  0.000000  ...        0          0   0.000000\n","lf_has_bad_words1          12      [1]  0.592330  ...      400         17   0.959233\n","lf_has_bad_words2          13      [1]  0.159091  ...       84         28   0.750000\n","lf_has_bad_words3          14       []  0.000000  ...        0          0   0.000000\n","lf_has_bad_words4          15       []  0.000000  ...        0          0   0.000000\n","lf_has_bad_words5          16      [1]  0.048295  ...       29          5   0.852941\n","lf_has_bad_words6          17      [1]  0.002841  ...        0          2   0.000000\n","lf_has_please              18      [1]  0.011364  ...        6          2   0.750000\n","lf_has_thankyou            19      [0]  0.012784  ...        5          4   0.555556\n","lf_all_capslock            20      [0]  0.002841  ...        2          0   1.000000\n","lf_has_angry_punctuations  21       []  0.000000  ...        0          0   0.000000\n","lf_has_url                 22      [1]  0.130682  ...       44         48   0.478261\n","lf_has_truncate            23      [1]  0.075284  ...       39         14   0.735849\n","lf_has_mention             24      [1]  0.593750  ...      298        120   0.712919\n","lf_has_retweet             25      [1]  0.241477  ...      140         30   0.823529\n","\n","[26 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Ly50JQOrFm1G"},"source":["# Save the unigram transformer for prediction\n","SAVE_PATH = '../models/'\n","\n","# Set save directory for transformer unigram\n","os.chdir(SAVE_PATH)\n","#model_name = 'lf_valid_final.pkl'\n","with open(model_name, 'wb') as file:\n","    pickle.dump(l_valid, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855},"id":"W8cG6HlnGA8L","executionInfo":{"status":"ok","timestamp":1624085971878,"user_tz":420,"elapsed":361,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"c6855510-e822-4cfc-ae12-9e7b5bd01b58"},"source":["# Create labeling matrix and evaluate results\n","#l_test = applier.apply(df_test)\n","\n","# Ignore all future warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","LFAnalysis(L=l_test, lfs=lfs).lf_summary(Y=df_test.label.values)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","      <th>Correct</th>\n","      <th>Incorrect</th>\n","      <th>Emp. Acc.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.618431</td>\n","      <td>5212</td>\n","      <td>1125</td>\n","      <td>0.822471</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.618431</td>\n","      <td>4862</td>\n","      <td>1475</td>\n","      <td>0.767240</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.782231</td>\n","      <td>0.782231</td>\n","      <td>0.477671</td>\n","      <td>3458</td>\n","      <td>1499</td>\n","      <td>0.697599</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.107306</td>\n","      <td>0.107306</td>\n","      <td>0.094209</td>\n","      <td>181</td>\n","      <td>499</td>\n","      <td>0.266176</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.187786</td>\n","      <td>0.187786</td>\n","      <td>0.123244</td>\n","      <td>799</td>\n","      <td>391</td>\n","      <td>0.671429</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.009468</td>\n","      <td>0.009468</td>\n","      <td>0.005681</td>\n","      <td>41</td>\n","      <td>19</td>\n","      <td>0.683333</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.070380</td>\n","      <td>0.070380</td>\n","      <td>0.055547</td>\n","      <td>241</td>\n","      <td>205</td>\n","      <td>0.540359</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.088685</td>\n","      <td>0.088685</td>\n","      <td>0.064384</td>\n","      <td>405</td>\n","      <td>157</td>\n","      <td>0.720641</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.105728</td>\n","      <td>0.105728</td>\n","      <td>0.067224</td>\n","      <td>491</td>\n","      <td>179</td>\n","      <td>0.732836</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.602335</td>\n","      <td>0.602335</td>\n","      <td>0.274420</td>\n","      <td>3630</td>\n","      <td>187</td>\n","      <td>0.951009</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.165694</td>\n","      <td>0.165694</td>\n","      <td>0.102888</td>\n","      <td>802</td>\n","      <td>248</td>\n","      <td>0.763810</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.001578</td>\n","      <td>0.001578</td>\n","      <td>0.000316</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.000473</td>\n","      <td>0.000473</td>\n","      <td>0.000473</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.035664</td>\n","      <td>0.035664</td>\n","      <td>0.020041</td>\n","      <td>196</td>\n","      <td>30</td>\n","      <td>0.867257</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.003314</td>\n","      <td>0.003314</td>\n","      <td>0.003314</td>\n","      <td>6</td>\n","      <td>15</td>\n","      <td>0.285714</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.007417</td>\n","      <td>0.007417</td>\n","      <td>0.005523</td>\n","      <td>30</td>\n","      <td>17</td>\n","      <td>0.638298</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.004734</td>\n","      <td>0.004734</td>\n","      <td>0.004576</td>\n","      <td>10</td>\n","      <td>20</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.001262</td>\n","      <td>0.001262</td>\n","      <td>0.000947</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.127505</td>\n","      <td>0.127505</td>\n","      <td>0.091053</td>\n","      <td>501</td>\n","      <td>307</td>\n","      <td>0.620050</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.066751</td>\n","      <td>0.066751</td>\n","      <td>0.045132</td>\n","      <td>299</td>\n","      <td>124</td>\n","      <td>0.706856</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.573615</td>\n","      <td>0.573615</td>\n","      <td>0.375414</td>\n","      <td>2653</td>\n","      <td>982</td>\n","      <td>0.729849</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.258482</td>\n","      <td>0.258482</td>\n","      <td>0.157961</td>\n","      <td>1311</td>\n","      <td>327</td>\n","      <td>0.800366</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  ...  Correct  Incorrect  Emp. Acc.\n","lf_clf_tfidf_bow            0   [0, 1]  1.000000  ...     5212       1125   0.822471\n","lf_clf_wordembed_nlp        1   [0, 1]  1.000000  ...     4862       1475   0.767240\n","lf_vader_sentiment          2   [0, 1]  0.782231  ...     3458       1499   0.697599\n","lf_emoji_sentiment_nlp      3   [0, 1]  0.107306  ...      181        499   0.266176\n","lf_has_person_nlp           4      [1]  0.187786  ...      799        391   0.671429\n","lf_has_work_art_nlp         5      [1]  0.009468  ...       41         19   0.683333\n","lf_has_3plus_entity_nlp     6      [1]  0.070380  ...      241        205   0.540359\n","lf_has_please_stop_nlp      7      [1]  0.088685  ...      405        157   0.720641\n","lf_has_stopwords_nlp        8      [1]  0.105728  ...      491        179   0.732836\n","lf_has_harassment_nlp       9       []  0.000000  ...        0          0   0.000000\n","lf_has_report_you_nlp      10       []  0.000000  ...        0          0   0.000000\n","lf_has_please_read_nlp     11       []  0.000000  ...        0          0   0.000000\n","lf_has_bad_words1          12      [1]  0.602335  ...     3630        187   0.951009\n","lf_has_bad_words2          13      [1]  0.165694  ...      802        248   0.763810\n","lf_has_bad_words3          14      [1]  0.001578  ...       10          0   1.000000\n","lf_has_bad_words4          15      [1]  0.000473  ...        2          1   0.666667\n","lf_has_bad_words5          16      [1]  0.035664  ...      196         30   0.867257\n","lf_has_bad_words6          17      [1]  0.003314  ...        6         15   0.285714\n","lf_has_please              18      [1]  0.007417  ...       30         17   0.638298\n","lf_has_thankyou            19      [0]  0.004734  ...       10         20   0.333333\n","lf_all_capslock            20      [0]  0.001262  ...        8          0   1.000000\n","lf_has_angry_punctuations  21       []  0.000000  ...        0          0   0.000000\n","lf_has_url                 22      [1]  0.127505  ...      501        307   0.620050\n","lf_has_truncate            23      [1]  0.066751  ...      299        124   0.706856\n","lf_has_mention             24      [1]  0.573615  ...     2653        982   0.729849\n","lf_has_retweet             25      [1]  0.258482  ...     1311        327   0.800366\n","\n","[26 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"dFYm_cZuGM93"},"source":["# Save the unigram transformer for prediction\n","SAVE_PATH = '../models/'\n","\n","# Set save directory for transformer unigram\n","os.chdir(SAVE_PATH)\n","#model_name = 'lf_test_final.pkl'\n","with open(model_name, 'wb') as file:\n","    pickle.dump(l_test, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855},"id":"kXX8THTd9Zvw","executionInfo":{"status":"ok","timestamp":1624085979003,"user_tz":420,"elapsed":946,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"6ca24d62-293f-484f-8ec5-08459ddac5f6"},"source":["# Create labeling matrix and evaluate results\n","#l_train = applier.apply(df_train)\n","\n","# Ignore all future warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","LFAnalysis(L=l_train, lfs=lfs).lf_summary(Y=df_train.label.values)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>j</th>\n","      <th>Polarity</th>\n","      <th>Coverage</th>\n","      <th>Overlaps</th>\n","      <th>Conflicts</th>\n","      <th>Correct</th>\n","      <th>Incorrect</th>\n","      <th>Emp. Acc.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lf_clf_tfidf_bow</th>\n","      <td>0</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.583798</td>\n","      <td>23451</td>\n","      <td>4509</td>\n","      <td>0.838734</td>\n","    </tr>\n","    <tr>\n","      <th>lf_clf_wordembed_nlp</th>\n","      <td>1</td>\n","      <td>[0, 1]</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.583798</td>\n","      <td>21645</td>\n","      <td>6315</td>\n","      <td>0.774142</td>\n","    </tr>\n","    <tr>\n","      <th>lf_vader_sentiment</th>\n","      <td>2</td>\n","      <td>[0, 1]</td>\n","      <td>0.782582</td>\n","      <td>0.782582</td>\n","      <td>0.448748</td>\n","      <td>15385</td>\n","      <td>6496</td>\n","      <td>0.703121</td>\n","    </tr>\n","    <tr>\n","      <th>lf_emoji_sentiment_nlp</th>\n","      <td>3</td>\n","      <td>[0, 1]</td>\n","      <td>0.053863</td>\n","      <td>0.053863</td>\n","      <td>0.046924</td>\n","      <td>348</td>\n","      <td>1158</td>\n","      <td>0.231076</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_person_nlp</th>\n","      <td>4</td>\n","      <td>[1]</td>\n","      <td>0.190558</td>\n","      <td>0.190558</td>\n","      <td>0.122568</td>\n","      <td>3516</td>\n","      <td>1812</td>\n","      <td>0.659910</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_work_art_nlp</th>\n","      <td>5</td>\n","      <td>[1]</td>\n","      <td>0.006867</td>\n","      <td>0.006867</td>\n","      <td>0.004614</td>\n","      <td>126</td>\n","      <td>66</td>\n","      <td>0.656250</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_3plus_entity_nlp</th>\n","      <td>6</td>\n","      <td>[1]</td>\n","      <td>0.073605</td>\n","      <td>0.073605</td>\n","      <td>0.055472</td>\n","      <td>1095</td>\n","      <td>963</td>\n","      <td>0.532070</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_stop_nlp</th>\n","      <td>7</td>\n","      <td>[1]</td>\n","      <td>0.094671</td>\n","      <td>0.094671</td>\n","      <td>0.064878</td>\n","      <td>1978</td>\n","      <td>669</td>\n","      <td>0.747261</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_stopwords_nlp</th>\n","      <td>8</td>\n","      <td>[1]</td>\n","      <td>0.100143</td>\n","      <td>0.100143</td>\n","      <td>0.060801</td>\n","      <td>2073</td>\n","      <td>727</td>\n","      <td>0.740357</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_harassment_nlp</th>\n","      <td>9</td>\n","      <td>[0]</td>\n","      <td>0.000465</td>\n","      <td>0.000465</td>\n","      <td>0.000429</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_report_you_nlp</th>\n","      <td>10</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please_read_nlp</th>\n","      <td>11</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words1</th>\n","      <td>12</td>\n","      <td>[1]</td>\n","      <td>0.603827</td>\n","      <td>0.603827</td>\n","      <td>0.242489</td>\n","      <td>16089</td>\n","      <td>794</td>\n","      <td>0.952970</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words2</th>\n","      <td>13</td>\n","      <td>[1]</td>\n","      <td>0.165343</td>\n","      <td>0.165343</td>\n","      <td>0.098748</td>\n","      <td>3502</td>\n","      <td>1121</td>\n","      <td>0.757517</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words3</th>\n","      <td>14</td>\n","      <td>[1]</td>\n","      <td>0.001109</td>\n","      <td>0.001109</td>\n","      <td>0.000501</td>\n","      <td>28</td>\n","      <td>3</td>\n","      <td>0.903226</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words4</th>\n","      <td>15</td>\n","      <td>[1]</td>\n","      <td>0.000322</td>\n","      <td>0.000322</td>\n","      <td>0.000250</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words5</th>\n","      <td>16</td>\n","      <td>[1]</td>\n","      <td>0.032332</td>\n","      <td>0.032332</td>\n","      <td>0.017382</td>\n","      <td>775</td>\n","      <td>129</td>\n","      <td>0.857301</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_bad_words6</th>\n","      <td>17</td>\n","      <td>[1]</td>\n","      <td>0.003255</td>\n","      <td>0.003255</td>\n","      <td>0.003112</td>\n","      <td>16</td>\n","      <td>75</td>\n","      <td>0.175824</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_please</th>\n","      <td>18</td>\n","      <td>[1]</td>\n","      <td>0.007403</td>\n","      <td>0.007403</td>\n","      <td>0.005794</td>\n","      <td>117</td>\n","      <td>90</td>\n","      <td>0.565217</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_thankyou</th>\n","      <td>19</td>\n","      <td>[0]</td>\n","      <td>0.005150</td>\n","      <td>0.005150</td>\n","      <td>0.004900</td>\n","      <td>77</td>\n","      <td>67</td>\n","      <td>0.534722</td>\n","    </tr>\n","    <tr>\n","      <th>lf_all_capslock</th>\n","      <td>20</td>\n","      <td>[0]</td>\n","      <td>0.001753</td>\n","      <td>0.001753</td>\n","      <td>0.001144</td>\n","      <td>41</td>\n","      <td>8</td>\n","      <td>0.836735</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_angry_punctuations</th>\n","      <td>21</td>\n","      <td>[]</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_url</th>\n","      <td>22</td>\n","      <td>[1]</td>\n","      <td>0.131652</td>\n","      <td>0.131652</td>\n","      <td>0.090165</td>\n","      <td>2198</td>\n","      <td>1483</td>\n","      <td>0.597120</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_truncate</th>\n","      <td>23</td>\n","      <td>[1]</td>\n","      <td>0.072711</td>\n","      <td>0.072711</td>\n","      <td>0.046853</td>\n","      <td>1442</td>\n","      <td>591</td>\n","      <td>0.709297</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_mention</th>\n","      <td>24</td>\n","      <td>[1]</td>\n","      <td>0.576681</td>\n","      <td>0.576681</td>\n","      <td>0.362375</td>\n","      <td>11627</td>\n","      <td>4497</td>\n","      <td>0.721099</td>\n","    </tr>\n","    <tr>\n","      <th>lf_has_retweet</th>\n","      <td>25</td>\n","      <td>[1]</td>\n","      <td>0.260300</td>\n","      <td>0.260300</td>\n","      <td>0.148247</td>\n","      <td>5850</td>\n","      <td>1428</td>\n","      <td>0.803792</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            j Polarity  Coverage  ...  Correct  Incorrect  Emp. Acc.\n","lf_clf_tfidf_bow            0   [0, 1]  1.000000  ...    23451       4509   0.838734\n","lf_clf_wordembed_nlp        1   [0, 1]  1.000000  ...    21645       6315   0.774142\n","lf_vader_sentiment          2   [0, 1]  0.782582  ...    15385       6496   0.703121\n","lf_emoji_sentiment_nlp      3   [0, 1]  0.053863  ...      348       1158   0.231076\n","lf_has_person_nlp           4      [1]  0.190558  ...     3516       1812   0.659910\n","lf_has_work_art_nlp         5      [1]  0.006867  ...      126         66   0.656250\n","lf_has_3plus_entity_nlp     6      [1]  0.073605  ...     1095        963   0.532070\n","lf_has_please_stop_nlp      7      [1]  0.094671  ...     1978        669   0.747261\n","lf_has_stopwords_nlp        8      [1]  0.100143  ...     2073        727   0.740357\n","lf_has_harassment_nlp       9      [0]  0.000465  ...       13          0   1.000000\n","lf_has_report_you_nlp      10       []  0.000000  ...        0          0   0.000000\n","lf_has_please_read_nlp     11       []  0.000000  ...        0          0   0.000000\n","lf_has_bad_words1          12      [1]  0.603827  ...    16089        794   0.952970\n","lf_has_bad_words2          13      [1]  0.165343  ...     3502       1121   0.757517\n","lf_has_bad_words3          14      [1]  0.001109  ...       28          3   0.903226\n","lf_has_bad_words4          15      [1]  0.000322  ...        6          3   0.666667\n","lf_has_bad_words5          16      [1]  0.032332  ...      775        129   0.857301\n","lf_has_bad_words6          17      [1]  0.003255  ...       16         75   0.175824\n","lf_has_please              18      [1]  0.007403  ...      117         90   0.565217\n","lf_has_thankyou            19      [0]  0.005150  ...       77         67   0.534722\n","lf_all_capslock            20      [0]  0.001753  ...       41          8   0.836735\n","lf_has_angry_punctuations  21       []  0.000000  ...        0          0   0.000000\n","lf_has_url                 22      [1]  0.131652  ...     2198       1483   0.597120\n","lf_has_truncate            23      [1]  0.072711  ...     1442        591   0.709297\n","lf_has_mention             24      [1]  0.576681  ...    11627       4497   0.721099\n","lf_has_retweet             25      [1]  0.260300  ...     5850       1428   0.803792\n","\n","[26 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"E9Y_sqUeJKnh"},"source":["# Save the unigram transformer for prediction\n","SAVE_PATH = '../models/'\n","\n","# Set save directory for transformer unigram\n","os.chdir(SAVE_PATH)\n","#model_name = 'lf_train_final.pkl'\n","with open(model_name, 'wb') as file:\n","    pickle.dump(l_train, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QwdSsXJ2YQM","colab":{"base_uri":"https://localhost:8080/","height":788},"executionInfo":{"status":"ok","timestamp":1624086008460,"user_tz":420,"elapsed":158,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"5c121299-1470-4fbb-db0b-165cc30183f3"},"source":["# Explore incorrect labels\n","pd.set_option('display.max_colwidth', None)\n","df_dev.iloc[l_dev[:,1] == HATE].sample(20, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>113</th>\n","      <td>1</td>\n","      <td>hahaha dumb bitch</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>1</td>\n","      <td>Fake ass people. that is why I do not have no respect for em. Frontin ass bitches too</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>I was planning on stopping by Portland again, but I do not think I'm going to have the time for that. Too much to do when I get back.</td>\n","    </tr>\n","    <tr>\n","      <th>123</th>\n","      <td>1</td>\n","      <td>#has_mention your a big yute to be having slits in your brows my darg lol drop it out</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>1</td>\n","      <td>&amp; I MOST DEFIANTLY WILL NOT let no bitch who FW a nigga I ALREADY HAD get the best out of me 😂😭 Fuccin wit my Hand-Me-Downs &amp; shit</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>1</td>\n","      <td>you bitches love yall some corny nigga</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>0</td>\n","      <td>NO NO NO NO NO #has_url STOP. Stop advocating that people pay for abuse mitigation.</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0</td>\n","      <td>We hope those gozlemes are worth the wait girls! #mkr #hungrycampers</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0</td>\n","      <td>My notifications are full of man tears! I thought I was going to have to wait till Christmas! Thank you baby Jesus!!</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>1</td>\n","      <td>#has_retweet #has_mention but naaww though, you all hoes is fu do not fucking play w/me.</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>1</td>\n","      <td>#has_mention pussy</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>1</td>\n","      <td>“ #has_mention Lool I agree RT“ #has_mention the most fucked up show I ever watched was misfits”” wen Simone wifed that hoe was the saddest</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>1</td>\n","      <td>#has_retweet #has_mention you all bitches got to stop pushing away the niggas who really care about you all for these niggas who do not give a fuck</td>\n","    </tr>\n","    <tr>\n","      <th>181</th>\n","      <td>1</td>\n","      <td>Just saw the baddest bitch in life walking down the street if she did not have that Aeropostale shirt on I prolly woulda gamed her ass up</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>#has_retweet #has_mention #has_mention #has_mention excellent short film by Eleonore Pourriat - everyday sexism to a man #has_url #has_truncate</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>0</td>\n","      <td>GamerGate loves to flood #has_mention so we will not see any legitimate harassment complaints that come in. :\\</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>1</td>\n","      <td>I do not know what you been told but I love them project hoes</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>1</td>\n","      <td>People hate on da rig life but I love my baby more then a pussy ass nigguh</td>\n","    </tr>\n","    <tr>\n","      <th>188</th>\n","      <td>1</td>\n","      <td>#has_retweet #has_mention fixing to fly me a Thot out here! Get back on My make bitches Fly like Peter Pan shit.</td>\n","    </tr>\n","    <tr>\n","      <th>175</th>\n","      <td>1</td>\n","      <td>omfg #has_retweet #has_mention Your feet bitch #has_retweet #has_mention Whisper something dirty in my ear 😋 #has_url</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     label                                                                                                                                                tweet\n","113      1                                                                                                                                    hahaha dumb bitch\n","142      1                                                                Fake ass people. that is why I do not have no respect for em. Frontin ass bitches too\n","10       0                I was planning on stopping by Portland again, but I do not think I'm going to have the time for that. Too much to do when I get back.\n","123      1                                                                #has_mention your a big yute to be having slits in your brows my darg lol drop it out\n","116      1                   & I MOST DEFIANTLY WILL NOT let no bitch who FW a nigga I ALREADY HAD get the best out of me 😂😭 Fuccin wit my Hand-Me-Downs & shit\n","146      1                                                                                                               you bitches love yall some corny nigga\n","96       0                                                                  NO NO NO NO NO #has_url STOP. Stop advocating that people pay for abuse mitigation.\n","38       0                                                                                 We hope those gozlemes are worth the wait girls! #mkr #hungrycampers\n","33       0                                 My notifications are full of man tears! I thought I was going to have to wait till Christmas! Thank you baby Jesus!!\n","119      1                                                             #has_retweet #has_mention but naaww though, you all hoes is fu do not fucking play w/me.\n","154      1                                                                                                                                   #has_mention pussy\n","131      1          “ #has_mention Lool I agree RT“ #has_mention the most fucked up show I ever watched was misfits”” wen Simone wifed that hoe was the saddest\n","145      1  #has_retweet #has_mention you all bitches got to stop pushing away the niggas who really care about you all for these niggas who do not give a fuck\n","181      1            Just saw the baddest bitch in life walking down the street if she did not have that Aeropostale shirt on I prolly woulda gamed her ass up\n","4        0      #has_retweet #has_mention #has_mention #has_mention excellent short film by Eleonore Pourriat - everyday sexism to a man #has_url #has_truncate\n","75       0                                       GamerGate loves to flood #has_mention so we will not see any legitimate harassment complaints that come in. :\\\n","129      1                                                                                        I do not know what you been told but I love them project hoes\n","163      1                                                                           People hate on da rig life but I love my baby more then a pussy ass nigguh\n","188      1                                     #has_retweet #has_mention fixing to fly me a Thot out here! Get back on My make bitches Fly like Peter Pan shit.\n","175      1                                omfg #has_retweet #has_mention Your feet bitch #has_retweet #has_mention Whisper something dirty in my ear 😋 #has_url"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"aiaeggFA2atW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624086062289,"user_tz":420,"elapsed":181,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"907dd39c-e879-45cc-c508-48125f8abb94"},"source":["print('Number of labeled examples:', len(df_dev.label),\n","      '\\nRatio of tweets labeled:', round(len(df_dev.label) / len(df_dev), 2),\n","      '\\nRatio positive:', round(\n","          len([val for val in df_dev.label if val == 1]) \n","          / len(df_dev.label), 2\n","          )\n","      )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of labeled examples: 200 \n","Ratio of tweets labeled: 1.0 \n","Ratio positive: 0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DSyD8OXa2dFk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624086080397,"user_tz":420,"elapsed":161,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"e4da2d75-0712-48bf-9a65-482247b4c5a9"},"source":["print(\"Overall, we reach a coverage ratio of\", round(\n","    LFAnalysis(L=l_dev, lfs=lfs).label_coverage(), 2), \n","    \"over the whole training set.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overall, we reach a coverage ratio of 1.0 over the whole training set.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WthZ8_jFEunU"},"source":[""],"execution_count":null,"outputs":[]}]}