{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ntbk_02_labeled_clean_data.ipynb","provenance":[{"file_id":"1V_EV8ieD4lpbhyX0Ze-MqPKqfbqKoh5Z","timestamp":1623541750199}],"collapsed_sections":[],"authorship_tag":"ABX9TyPAxGLGt8sGX6OI08i+RdZV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qXBhzbcpQaQV"},"source":["# Dive into Abusive Language with Snorkel\n","\n","Author: BingYune Chen \n","<br>\n","Updated: 2021-08-02\n","\n","----------"]},{"cell_type":"markdown","metadata":{"id":"imVvTC7k-qKy"},"source":["### Data Preparation\n","\n","We applied an extensive set of pre-processing steps to decrease the size of the feature set, making it more suitable for learning algorithms.\n","\n","* Remove HTML entities such as '&lt' or '&amp'\n","* Identify general tweet elements such as retweets, urls, and mentions\n","* Expand contractions"]},{"cell_type":"code","metadata":{"id":"GFPLaKiSoKCH"},"source":["# Imports and setup for Google Colab\n","\n","# Mount Google Drive\n","import os, sys # interact with Google Drive's operating system\n","from google.colab import drive ## module to use Google Drive with Python\n","drive.mount('/content/drive') ## mount to access contents\n","\n","# Install python libraries\n","! pip install twarc --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNMOK0VcqCvI"},"source":["# Load standard libraries\n","import numpy as np\n","import pandas as pd\n","\n","import re\n","import itertools \n","\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYD4pNZdpH9j"},"source":["# Load combined labeled dataset\n","df = pd.read_csv('../data/interim/labeled_combined_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"j_1UBqQ74aG-","executionInfo":{"status":"ok","timestamp":1623631799545,"user_tz":420,"elapsed":404,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"82433cfa-7816-43b7-9efd-b4e3b33fe9d5"},"source":["# Confirm data loaded correctly with 'label' and 'tweet' columns\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>RT @ing3nu: Packed house for #WIG2015 http://t...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>I need to stop frantically typing up responses...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>For the first time in my months of monitoring ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@holinka ARE YOU AT GDC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n","1      0  RT @ing3nu: Packed house for #WIG2015 http://t...\n","2      0  I need to stop frantically typing up responses...\n","3      0  For the first time in my months of monitoring ...\n","4      0                            @holinka ARE YOU AT GDC"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"mPQlnFivaibN"},"source":["# Replace contractions \n","# Code adapted from https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597\n","# Contractions source https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n","def load_dict_contractions():\n","    return {\n","        \"ain't\":\"is not\",\n","        \"amn't\":\"am not\",\n","        \"aren't\":\"are not\",\n","        \"can't\":\"cannot\",\n","        \"'cause\":\"because\",\n","        \"couldn't\":\"could not\",\n","        \"couldn't've\":\"could not have\",\n","        \"could've\":\"could have\",\n","        \"daren't\":\"dare not\",\n","        \"daresn't\":\"dare not\",\n","        \"dasn't\":\"dare not\",\n","        \"didn't\":\"did not\",\n","        \"doesn't\":\"does not\",\n","        \"don't\":\"do not\",\n","        \"e'er\":\"ever\",\n","        \"em\":\"them\",\n","        \"everyone's\":\"everyone is\",\n","        \"finna\":\"fixing to\",\n","        \"gimme\":\"give me\",\n","        \"gonna\":\"going to\",\n","        \"gon't\":\"go not\",\n","        \"gotta\":\"got to\",\n","        \"hadn't\":\"had not\",\n","        \"hasn't\":\"has not\",\n","        \"haven't\":\"have not\",\n","        \"he'd\":\"he would\",\n","        \"he'll\":\"he will\",\n","        \"he's\":\"he is\",\n","        \"he've\":\"he have\",\n","        \"how'd\":\"how would\",\n","        \"how'll\":\"how will\",\n","        \"how're\":\"how are\",\n","        \"how's\":\"how is\",\n","        \"I'd\":\"I would\",\n","        \"I'll\":\"I will\",\n","        \"I'm\":\"I am\",\n","        \"I'm'a\":\"I am about to\",\n","        \"I'm'o\":\"I am going to\",\n","        \"isn't\":\"is not\",\n","        \"it'd\":\"it would\",\n","        \"it'll\":\"it will\",\n","        \"it's\":\"it is\",\n","        \"I've\":\"I have\",\n","        \"kinda\":\"kind of\",\n","        \"let's\":\"let us\",\n","        \"mayn't\":\"may not\",\n","        \"may've\":\"may have\",\n","        \"mightn't\":\"might not\",\n","        \"might've\":\"might have\",\n","        \"mustn't\":\"must not\",\n","        \"mustn't've\":\"must not have\",\n","        \"must've\":\"must have\",\n","        \"needn't\":\"need not\",\n","        \"ne'er\":\"never\",\n","        \"o'\":\"of\",\n","        \"o'er\":\"over\",\n","        \"ol'\":\"old\",\n","        \"oughtn't\":\"ought not\",\n","        \"shalln't\":\"shall not\",\n","        \"shan't\":\"shall not\",\n","        \"she'd\":\"she would\",\n","        \"she'll\":\"she will\",\n","        \"she's\":\"she is\",\n","        \"shouldn't\":\"should not\",\n","        \"shouldn't've\":\"should not have\",\n","        \"should've\":\"should have\",\n","        \"somebody's\":\"somebody is\",\n","        \"someone's\":\"someone is\",\n","        \"something's\":\"something is\",\n","        \"that'd\":\"that would\",\n","        \"that'll\":\"that will\",\n","        \"that're\":\"that are\",\n","        \"that's\":\"that is\",\n","        \"there'd\":\"there would\",\n","        \"there'll\":\"there will\",\n","        \"there're\":\"there are\",\n","        \"there's\":\"there is\",\n","        \"these're\":\"these are\",\n","        \"they'd\":\"they would\",\n","        \"they'll\":\"they will\",\n","        \"they're\":\"they are\",\n","        \"they've\":\"they have\",\n","        \"this's\":\"this is\",\n","        \"those're\":\"those are\",\n","        \"'tis\":\"it is\",\n","        \"'twas\":\"it was\",\n","        \"wanna\":\"want to\",\n","        \"wasn't\":\"was not\",\n","        \"we'd\":\"we would\",\n","        \"we'd've\":\"we would have\",\n","        \"we'll\":\"we will\",\n","        \"we're\":\"we are\",\n","        \"weren't\":\"were not\",\n","        \"we've\":\"we have\",\n","        \"what'd\":\"what did\",\n","        \"what'll\":\"what will\",\n","        \"what're\":\"what are\",\n","        \"what's\":\"what is\",\n","        \"what've\":\"what have\",\n","        \"when's\":\"when is\",\n","        \"where'd\":\"where did\",\n","        \"where're\":\"where are\",\n","        \"where's\":\"where is\",\n","        \"where've\":\"where have\",\n","        \"which's\":\"which is\",\n","        \"who'd\":\"who would\",\n","        \"who'd've\":\"who would have\",\n","        \"who'll\":\"who will\",\n","        \"who're\":\"who are\",\n","        \"who's\":\"who is\",\n","        \"who've\":\"who have\",\n","        \"why'd\":\"why did\",\n","        \"why're\":\"why are\",\n","        \"why's\":\"why is\",\n","        \"won't\":\"will not\",\n","        \"wouldn't\":\"would not\",\n","        \"would've\":\"would have\",\n","        \"y'all\":\"you all\",\n","        \"you'd\":\"you would\",\n","        \"you'll\":\"you will\",\n","        \"you're\":\"you are\",\n","        \"you've\":\"you have\",\n","        \"Whatcha\":\"What are you\",\n","        \"luv\":\"love\",\n","        \"sux\":\"sucks\"\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnbWE-sDQB8u"},"source":["# Clean tweet text to remove mentions, retweets, urls\n","def clean_tweet_txt(tweet_txt):\n","    \n","    ## remove mentions, but keep hashtags\n","    tweet_txt = ' '.join(re.sub(\n","        '(@[A-Za-z0-9_]+\\:)|(@[A-Za-z0-9_\\.]+)', \n","        ' #has_mention ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    ## remove retweets\n","    tweet_txt = ' '.join(re.sub(\n","        '(RT\\: )|(RT\\:)|(RT \\: )|(RT )', \n","        ' #has_retweet ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    ## remove punctuation not needed for VADER sentiment\n","    tweet_txt = ' '.join(re.sub(\n","        '\\\\.\\\\.\\\\.$|[@…]', \n","        ' #has_truncate ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    ## remove urls\n","    tweet_txt = ' '.join(re.sub(\n","        '(\\w+:\\/\\/\\S+)|(\\w+:)', \n","        ' #has_url ', \n","        tweet_txt\n","        ).split()\n","    )\n","    \n","    ## expand contractions\n","    CONTRACTIONS = load_dict_contractions()\n","    tweet_txt = tweet_txt.replace(\"’\",\"'\")\n","    words = tweet_txt.split()\n","    reformed = [CONTRACTIONS[\n","        word.lower()] if word.lower() in CONTRACTIONS else word \n","        for word in words\n","        ]\n","    tweet_txt = \" \".join(reformed)\n","    \n","    ## fix simple misspelled words (character repeats more than 2x)\n","    tweet_txt = ''.join(\n","        ''.join(t)[:2] for _, t in itertools.groupby(tweet_txt)\n","        )\n","    return tweet_txt\n","\n","df['tweet'] = df['tweet'].apply(clean_tweet_txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"nRlH1CPY_2fT","executionInfo":{"status":"ok","timestamp":1623631877759,"user_tz":420,"elapsed":2619,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"55dfbb22-5322-4af5-9cf6-9dbbae58c446"},"source":["# Check cleaned labeled dataset\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>#has_retweet #has_mention Packed house for #WI...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>I need to stop frantically typing up responses...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>For the first time in my months of monitoring ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>#has_mention ARE YOU AT GDC</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n","1      0  #has_retweet #has_mention Packed house for #WI...\n","2      0  I need to stop frantically typing up responses...\n","3      0  For the first time in my months of monitoring ...\n","4      0                        #has_mention ARE YOU AT GDC"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"EwdnEAU-wJ_d"},"source":["# Save cleaned labeled dataset\n","df.to_csv('../data/interim/labeled_cleaned_data.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYcElzPWiBBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623546061459,"user_tz":420,"elapsed":361,"user":{"displayName":"BingYune Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9vmLT0W2zQ1nvJObMkPJzKm9UxHOOtvLXOr1g4g=s64","userId":"01631520334329415750"}},"outputId":"5e339d5f-0820-4bdd-ac45-9c35a903c3aa"},"source":["# Create four subsets for training, validation, testing, development\n","\n","# Training set is dataset to be labeled by snorkel\n","# Testing set is used to evaluate the classifier scores\n","# Evaluate trained classifer on data neither used for labeling nor training\n","df_train, df_test = train_test_split(df.copy(), \n","                                     test_size=0.2, \n","                                     stratify=df['label'], \n","                                     random_state=42)\n","\n","# Development set is used to evaluate and optimize labeling functions\n","# Show accuracy of the labeling functions via LFAnalysis\n","df_dev = df_train.groupby('label').apply(\n","    lambda x: x.sample(100, random_state=42)).reset_index(level=0, drop=True)\n","df_train.drop(df_dev.index, inplace=True)\n","\n","# Validation set is used to evaluate the label model's predictions\n","# Show accuracy of generative approach via LabelModel\n","df_valid = df_test.sample(frac=0.1, random_state=42)\n","df_test.drop(df_valid.index, inplace=True)\n","\n","print('Train:', len(df_train), \n","      '\\t Dev:', len(df_dev), \n","      '\\t', 'Valid:', len(df_valid),\n","      '\\t Test:', len(df_test)\n","     ) # Train: 27960, Dev: 200, Valid: 704, Test: 6337"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train: 27960 \t Dev: 200 \t Valid: 704 \t Test: 6337\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eol59i8piB6-"},"source":["# Read to pickle files for cleaned, subset data\n","df_train.to_pickle('../data/processed/df_train.pkl')\n","df_dev.to_pickle('../data/processed/df_dev.pkl')\n","df_valid.to_pickle('../data/processed/df_valid.pkl')\n","df_test.to_pickle('../data/processed/df_test.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mB1AFscn49L_"},"source":["# Preprocess Twitter Sentiment140 dataset for transfer learning\n","\n","df_twitter140_full = pd.read_csv(\n","    '../data/external/training.1600000.processed.noemoticon.csv',\n","    engine='python',\n","    names=['target', 'id', 'date', 'flag', 'user', 'text']\n","    )\n","\n","df_twitter140_full['tweet'] = df_twitter140_full['text'].apply(clean_tweet_txt)\n","df_twitter = df_twitter140_full.loc[:, ['target', 'tweet']]\n","\n","df_twitter.to_pickle('../data/processed/twitter_sentiment140.pkl')"],"execution_count":null,"outputs":[]}]}